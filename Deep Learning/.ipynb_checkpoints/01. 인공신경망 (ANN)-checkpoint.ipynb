{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aritificial Neural network (ANN)\n",
    "* 함수 F가 신경망을 모사한 형태 <br/><br/>\n",
    "$Y=F(X)$\n",
    "![](./img/01_nn_cat.png)\n",
    "\n",
    "### Logistic Regression & ANN\n",
    "\n",
    "* Hidden layer 가 없는 신경망 모델은 Logistic regression / Linear regression 과 동일\n",
    "* For classficiation : $\\hat{Y} = Activation(W_{1}X_{1} + W_{2}X_{2} + W_{3}X_{3} +b)$\n",
    "* For regression, :  $\\hat{Y} = W_{1}X_{1} + W_{2}X_{2} + W_{3}X_{3} +b$\n",
    "* W : weight, 학습해야 할 모델의 파라미터\n",
    "\n",
    "![](./img/01_LR_nn.png)\n",
    "\n",
    "### Single-Hidden-Layer ANN\n",
    "\n",
    "* 입력변수의 비선형 변환들의 조합으로 예측을 수행하는 모델\n",
    "* Output layer : activation function (Task에 따라 결정) *linear, sigmoid, softmax*\n",
    "* Hidden layer : activation function (자유롭게 결정) *linear, sigmoid, softmax, relu, tanh...*\n",
    "* Input lyaer <br/><br/>\n",
    "$Y_{i} = Activation(W_{1i}^{(2)}H_{1} + W_{2i}^{(2)}H_{2} + b^{(2)})$ <br/><br/>\n",
    "$H_{i} = Activation(W_{1i}^{(1)}X_{1} + W_{2i}^{(1)}X_{2} + W_{3i}^{(1)}X_{3} + b^{(1)})$\n",
    "![](./img/01_single_hidden_layer.png)\n",
    "\n",
    "## Activation Function: Hidden Layer\n",
    "\n",
    "* 입력변수의 비선형 변환으로 새로운 변수를 재탄생 시키는 과정\n",
    "* https://en.wikipedia.org/wiki/Activation_function\n",
    "* 대표적으로 많이 사용하는 activation faunctions\n",
    "  * sigmoid\n",
    "  * tanh\n",
    "  * relu\n",
    "![](./img/01_activation_function.png)\n",
    "<br/><br/>\n",
    "  \n",
    "## Activation Function: Output Layer\n",
    "\n",
    "* 모델의 출력값을 확률로 전환하기 위해\n",
    "* Two-class classification: $f(x)=\\frac{1}{1+e^{-x}} $ ... sigmoid function\n",
    "* Multiclass classification: $f(x_{j})=\\frac{e^{x_{j}}}{\\sum_{i}1+e^{x_{i}}} $ ... softmax function\n",
    "\n",
    "![](./img/01_af_ol.png)\n",
    "\n",
    "\n",
    "### ANN 학습 : W찾기\n",
    "* 데이터를 정답으로 간주하고 모델 학습\n",
    "* 파라미터\n",
    "  * Parameter: 학습 가능 (e.g. weight, bias)\n",
    "  * Hyper parameter: 사용자가 임의로 결정 (e.g. 레이어의 수, 유닛의 수) <br/><br/>\n",
    "$arg_{W,b} min \\sum_{i}L(Y, F(X; W,b))$\n",
    "\n",
    "![](./img/01_find_w.png)\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "* 예측 값과 실제 값의 차이를 측정하는 방법\n",
    "* Regression: mean squared error (MSE)\n",
    "* 정의한 Loss function은 항상 미분가능해야 함\n",
    "  * ~~Regression: MAE (mean absolute error), MAPE (mean absolute percentage error)~~\n",
    "\n",
    "|예측값|실제값|\n",
    "|:----:|:----:|\n",
    "|11|10|\n",
    "|19|20|\n",
    "|30|30|\n",
    "|43|40|\n",
    "|47|50|\n",
    "\n",
    "$\\frac{1}{5}\\sum^{5}_{i=1}(y_{i} - \\hat{y}_{i})^{2}$ <br/>\n",
    "$=\\frac{1}{5}[(10-11)^{2} + (20-19)^{2} + (30-30)^{2} + (40-43)^{2}+(50-47)^{2}]$ <br/>\n",
    "$=\\frac{1}{5}(1+1+0+9+9) = 4$ <br/><br/>\n",
    "\n",
    "\n",
    "* Classficiation: cross entropy\n",
    "* 정의한 Loss function 이 미분 가능해야 함\n",
    "  * ~~Classfication: 0/1-Loss (Acc.)~~\n",
    "\n",
    "|예측값||| |실제값|||\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "|Class1|Class2|Class3| |Class1|Class2|Class3|\n",
    "|0.3|0.2|0.5| |1|0|0|\n",
    "|0.1|0.8|0.1| |0|1|0|\n",
    "|0.6|0.2|0.2| |1|0|0|\n",
    "|0.1|0.5|0.4| |0|0|1|\n",
    "\n",
    "* 실제값 : One-hot vector 형태로 해당 class 만 1로 표기함 (Demension을 맞춰주기 위해)\n",
    "\n",
    "$-\\sum^{4}_{i=1} ln(\\hat(y)_{i}) * y_{i} = -[ln(0.3) + ln(0.8) + ln(0.6) + ln(0.4)] = 2.85$ <br/><br/>\n",
    "\n",
    "\n",
    "## Gradient Decent Search\n",
    "\n",
    "* Numeric Optimization method\n",
    "* 최적해를 보장하지 않음\n",
    "* $\\nabla F (w_{1}, w_{2}, w_{3}) = \\frac{\\partial F}{\\partial W_{1}}e_{w_{1}} + \\frac{\\partial F}{\\partial W_{2}}e_{w_{2}} + \\frac{\\partial F}{\\partial W_{3}}e_{w_{3}}$ : F의 기울기가 가장 큰 방향\n",
    "\n",
    "![](./img/01_gradient_decent.png)\n",
    "\n",
    "$$w \\leftarrow w - \\eta\\nabla J(W)$$<br/><br/>\n",
    "\n",
    "\n",
    "##  $\\nabla J (Y,F(X;W))$\n",
    "\n",
    "* Loss function 에 업데이트 해야할 파라미터는 어디에? <br/>\n",
    "$(y_{i} - \\hat{y}_{i})^{2} ; MSE$ <br/>\n",
    "$=(y_{i} - Activation(W_{1}^{(2)}H_{1} + W_{2}^{(2)}H_{2}+b^{2}))^{2}$ <br/>\n",
    "$=(y_{i} - Activation(W_{1}^{(2)} Activation(W_{11}^{(1)}X_{1}+W_{21}^{(1)}X_{2}+W_{31}^{(1)}X_{x}+b_{1}^{(1)} ...))^{2}$ <br/>\n",
    "\n",
    "\n",
    "* Loss function 안에 모두 포함되어 있음 -> Loss function을 업데이트할 파라미터에 대해 미분을 계산만해주면 됨<br/><br/>\n",
    "\n",
    "\n",
    "## 합성함수로 표현한 신경망 모형\n",
    "\n",
    "* 선형조합 -> 비선형 변환 -> 선형조합 -> 비선형 변환 -> ... <br/><br/>\n",
    "$H_{i} = Activation(W_{1i}^{(1)}X_{1}+W_{2i}^{(1)}X_{2}+W_{3i}^{(1)}X_{x}+b_{i}^{(1)} ...)$ <br/>\n",
    "\n",
    "![](./img/01_activation_function_02.png)\n",
    "\n",
    "\n",
    "\n",
    "## 행렬과 합성함수로 표현한 신경망 모형\n",
    "\n",
    "* 모델을 보다 간단하게 표현하기 위해 <br/><br/>\n",
    "$$\\hat{y}=F(X;W) = \\sigma(HW^{(2)})=\\sigma(HW^{(1)})W^{(2)}$$\n",
    "\n",
    "\n",
    "## Chain Rule\n",
    "\n",
    "### 1. 합성함수의 미분규칙\n",
    "\n",
    "$$F(x) = f_{1}(f_{2}(f_{3} \\cdots f_{n}(x))) = f_{1}^{o} f_{2}^{o} \\cdots^{o} f_{n}(x)$$ \n",
    "<br/>\n",
    "$$\\frac{\\partial F}{\\partial a} = \\frac{\\partial F}{\\partial f_{1}} \\frac{\\partial f_{1}}{\\partial f_{2}} \\frac{\\partial f_{2}}{\\partial f_{3}} \\cdots \\frac{\\partial f_{n-1}}{\\partial f_{n}}$$\n",
    "<br/>\n",
    "\n",
    "### 2. Cross entropy loss function & ANN 예제\n",
    "\n",
    "$$\\hat{y} = F(X;W) = \\sigma (\\sigma(XW^{(1)})W^{(2)}) = \\sigma^{o}f_{2}^{o}\\sigma^{o}f_{1}(X)$$ \n",
    "<br/>\n",
    "$$L(W) = y ln \\hat(y) ; cross entropy$$\n",
    "<br/>\n",
    "$$\\frac{\\partial L}{\\partial f_{2}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial f_{2}} $$\n",
    "<br/>\n",
    "$$\\frac{\\partial L}{\\partial f_{1}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial f_{2}} \\frac{\\partial f_{2}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial f_{1}}$$\n",
    "\n",
    "\n",
    "## Back Propagation Algorithm\n",
    "\n",
    "* 틀린 예측을 바로잡기 위해 파라미터를 순차적으로 수정해 나가는 학습 알고리즘\n",
    "* net: activation function 이전의 값 (=선형변환)\n",
    "![](./img/01_back_propagation_01.png)\n",
    "\n",
    "* 아랫단의 경우 학습이 어려운 문제 : vanishing gradient problem\n",
    "\n",
    "### Back Propagation Algorithm\n",
    "- Gradient 의 규칙성\n",
    "- 반복적으로 나타나는 부분은 프로그래밍 시 효율적으로 처리해야 함\n",
    "- Theano/tensorflow/torch 등의 라이브러리는 gradient를 간편하게 계산해주는 기능을 포함\n",
    "\n",
    "![](./img/01_back_propagation_02.png)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## 신경망 모델 학습 전략\n",
    "* Stochastic gradient descent search\n",
    "* Batch size & Epoch\n",
    "* Early Stopping\n",
    "* Learning rate\n",
    "* Momentum\n",
    "* 여러가지 학습 알고리즘\n",
    "\n",
    "## Stochastic Gradient Descent Search\n",
    "\n",
    "* 온라인 학습 방식으로 나번에 한 개의 샘플에 대해서 모델 업데이트\n",
    "![](./img/01_stochastic_gradient_desent.png) \n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "## Batch Size & Epoch\n",
    "\n",
    "* Batch size : 한번에 업데이트할 샘플의 수\n",
    "  * Mini-batch gradient descent: 128 samples\n",
    "  * computation performance를 위해서 : batch size를 어느 정도 크게 하는 것\n",
    "  * batch size가 클 수록 계산은 빠르지만, 모델의 성능은 안 좋아질 수 있음\n",
    "\n",
    "* Epoch : 전체 데이터를 반복한 횟수\n",
    "  * 어느 정도 모델의 결과가 좋을 떄까지 반복\n",
    "  ![](./img/01_epoch.png) <br/><br/>\n",
    "  \n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "* 학습과정을 모니터링하여 조기에 학습 종료 (overfitting 방지)\n",
    "* 현대적 기계학습 방법론은 학습 성능이 아닌 예측 성능을 우선시 함\n",
    "* 따라서 테스트 데이터의 성능에 집중\n",
    "  ![](./img/01_early_stopping.png)\n",
    "  <br/><br/>\n",
    "\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "* 클 수록 모델 업데이트 시, 현재 에러를 크게 반영\n",
    "* 작을 수록 모델을 조금씩 업데이트\n",
    "* 일반적으로 0.01 ~ 0.001 정도로 설정\n",
    "* Tip) 학습이 잘 안될 경우 0.001 이하로 매우 작게 설정 <br/><br/>\n",
    "$$W^{(i)} \\ W^{(i)} - \\eta\\frac{\\partial L}{\\partial W^{(i)}}$$\n",
    "\n",
    "![](./img/01_learning_rate.png) \n",
    "<br/><br/>\n",
    "\n",
    "## Momentum\n",
    "\n",
    "* Local optimal 을 벗어나기 위한 학습 전략\n",
    "* $\\alpha$ 는 0에서 1 사이의 값으로 설정, 주로 0.9\n",
    "* $\\alpha$ 값이 클 수록 나쁜 local optimal을 잘 벗어나지만, 반대로 좋은 local optimal을 지나기도 쉬움 <br/><br/>\n",
    "$$W^{(i)}  W^{(i)} - (\\alpha \\Delta W^{(i)} + \\eta \\nabla L)$$\n",
    "\n",
    "![](./img/01_momentum.png)\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "## Gradient Decent Search\n",
    "\n",
    "* Learning rate, momentum을 어떻게 구성하는지에 따라 다양한 학습알고리즘이 존재\n",
    "* 최근에는 Adam을 많이 사용\n",
    "\n",
    "![](./img/01_adam.png)\n",
    "<br/><br/>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## ANN의 이론적 토대\n",
    "\n",
    "* Universal approximation theorem\n",
    "* 하나의 은닉층을 갖는 인공신경망은 임의의 연속인 다변수 함수를 원하는 정도의 정확도로 근사할 수 있음\n",
    "* 성능이 좋지 않다면 데이터, 학습방법이 문제!\n",
    "\n",
    "## Regularization on ANN\n",
    "* Training 정확도에서 손해를 보더라도 test(예측)을 잘 할 수 있도록 하는 방법\n",
    "* 모델의 generalization 성능을 향상시키는 방법 (variance 감소)\n",
    "  * neural net 은 variance 가 큰 모델이어서 variance 를 줄여서 최적의 모델을 찾기 위한 방법을 regularization 이라 함\n",
    "* variance와 bias 가 어느정도 trad-off 관계가 있음\n",
    "\n",
    "![](./img/01_regularization_on_ANN.png)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "### 1. Weight Decay\n",
    "* variance를 줄이는 방법 중 하나\n",
    "* 모델의 파라미터 W의 크기를 줄이는 제약을 추가\n",
    "* 모델의 복잡도를 감소 <br/><br/>\n",
    "$$arg_{w,b} min \\sum_{i} L(Y,F(X; W, b)) + \\lambda_{1} ||W||^{2}_{2} + \\lambda_{2} ||W||_{1}$$\n",
    "\n",
    "![](./img/01_weight_decay.png) <br/>\n",
    "* 가중치를 너무 크게 주면 대부분의 weight가 0으로 수렴하게 됨: 따라서 보통, 10-4E 정도로 값을 설정하게 됨\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "### 2. Data Augmentation\n",
    "\n",
    "* 학습데이터의 수를 늘려보자\n",
    "  * 랜덤한 노이즈를 데이터에 주입\n",
    "  * 이미지의 경우 위치이동, 좌우대칭, (해상도) 확대/축소 등의 변형을 생성\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
