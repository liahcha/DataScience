{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 실습 데이터\n",
    "\n",
    "* Handwriting Digit Recognition\n",
    "  * 28 * 28, 전체 784개 변수\n",
    "  * 0~9 로 구성된 10개의 클래스\n",
    "  \n",
    "  * training data : 60,000\n",
    "  * testing data : 10,000\n",
    "  \n",
    "## 1. 구조생성\n",
    "\n",
    "![](./img/01_example_01.png)\n",
    "\n",
    "* Dense(500) : 500개의 output\n",
    "* Activation function이 여러개인데 : 이 중, sigmoid를 생성\n",
    "\n",
    "* 500개가 들어가서 500개가 나옴 (sigmoid)를 통과해서 output을 내보내는 형태\n",
    "\n",
    "* 최종 레이어는 Dense(10) 10개의 output (0,1,2, ... 9 )\n",
    "* 최종 classification을 하기 때문에 'softmax'를 사용 \n",
    "  * binary classfication 일때는 'sigmoid'를 사용하고\n",
    "  * 일반적으로 2개 이상의 class 일 경우는 'softmax'를 사용함\n",
    "  \n",
    "## 2. Loss function 설정\n",
    "> model.compile(loss = 'categorical crossentropy', <br/>\n",
    ">              optimizer = 'adam', <br/>\n",
    ">              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "![](./img/01_example_02.png)\n",
    "\n",
    "* loss 와 optimizer는 학습이 되는 방향을 정해 줌\n",
    "* Availale loss function\n",
    "  * mean_squared_error\n",
    "  * mean_absolute_error\n",
    "  * mean_absolute_percentage_error\n",
    "  ...\n",
    "  * categorical_crossentropy\n",
    "  * binary_crossentropy\n",
    "  \n",
    "  * Regression 문제\n",
    "    * MSE 를 많이 씀 \n",
    "    * MAP (if MAP=10 이면, 10% 오차가 있다)\n",
    "  \n",
    "  * Classficiation 문제 : crossentropy\n",
    "\n",
    "\n",
    "* Available Optimizer\n",
    "  * SGD, RMSprpop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "  * 현재까지는 Adam 을 가장 많이 사용함\n",
    "  \n",
    "\n",
    "## 3. 모델링\n",
    "\n",
    "> model.fit(x_train, y_train, batch_size = 100, nb_epoch = 20)\n",
    "\n",
    "![](./img/01_example_03.png)\n",
    "\n",
    "![](./img/01_example_04.png)\n",
    "\n",
    "* classfication 이기 때문에,  one-hot vector 형식으로 y를 변형시켜 줌 \n",
    "  \n",
    "\n",
    "## 4. 모델 저장 및 로드\n",
    "\n",
    "```{python}\n",
    "## import modul\n",
    "from keras.models import load_model\n",
    "\n",
    "## save model\n",
    "model.save('path/filename.h5') \n",
    "\n",
    "## load model\n",
    "model = load_model('path/filename.h5')\n",
    "```\n",
    "\n",
    "## 5. 테스팅\n",
    "\n",
    "```\n",
    "## case1\n",
    "score = model.evaluation(x_test, y_test)\n",
    "print('Total loss on Testing set : ', score[0])\n",
    "print('Accuracy of Testing set : ', score[1])\n",
    "\n",
    "## case2\n",
    "result = model.predict(x_test)\n",
    "```\n",
    "\n",
    "\n",
    "## 6. batch size 및 epoch 설정\n",
    "\n",
    "* 데이터 사이즈가 커질 수록, batch size를 줄이던지, 사용하는 데이터에 맞게 변경\n",
    "\n",
    "![](./img/01_example_05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN ver1 실습\n",
    "\n",
    "- spyder에서 실행 시, variable explorer 로 편리하게 확인 가능함 \n",
    "<br/>\n",
    "![](./img/01_spyder_01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0  ~  1.0\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "x_train shape: (60000, 784)\n",
      "x_test shape: (10000, 784)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load modulas\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "#from keras.regularizers import # regulizer를 위한 설정\n",
    "\n",
    "## parameter\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epoch = 20\n",
    "\n",
    "## Load mnist data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "## 나누기 연산이 들어가므로 uint8 -> float32로 변경\n",
    "\n",
    "## preprocessing (scale)\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(np.min(x_train), ' ~ ', np.max(x_train))\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "# print('y_train shape:', y_train.shape)\n",
    "# print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## convert class vectors \n",
    "# Lable의 categorical 값을 One-hot 형태로 변환 \n",
    "# 예를 들어 [1, 3, 2, 0] 를 \n",
    "# [[ 0., 1., 0., 0.], \n",
    "# [ 0., 0., 0., 1.], \n",
    "# [ 0., 0., 1., 0.], \n",
    "# [ 1., 0., 0., 0.]] \n",
    "# 로 변환하는 것을 One-hot 형태라고 함\n",
    "## convert class vectors\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "#### deep neural network modeling \n",
    "info_input = Input(shape=(28*28,))\n",
    "\n",
    "layer1 = Dense(512)(info_input)\n",
    "layer1_act = Activation('relu')(layer1)\n",
    "\n",
    "layer2 = Dense(512)(layer1_act)\n",
    "layer2_act = Activation('relu')(layer2)\n",
    "\n",
    "layer3 = Dense(10)(layer2_act)\n",
    "layer3_act = Activation('softmax')(layer3)\n",
    "\n",
    "model = Model(inputs=[info_input], \n",
    "              outputs = [layer3_act])\n",
    "\n",
    "### model structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 33s - loss: 0.2205 - acc: 0.9354 - val_loss: 0.1025 - val_acc: 0.9669\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0797 - acc: 0.9756 - val_loss: 0.0745 - val_acc: 0.9767\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0507 - acc: 0.9839 - val_loss: 0.0709 - val_acc: 0.9767\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0354 - acc: 0.9885 - val_loss: 0.0647 - val_acc: 0.9802\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0285 - acc: 0.9906 - val_loss: 0.0678 - val_acc: 0.9804\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0666 - val_acc: 0.9808\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0196 - acc: 0.9932 - val_loss: 0.0694 - val_acc: 0.9809\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0758 - val_acc: 0.9789\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0171 - acc: 0.9945 - val_loss: 0.0785 - val_acc: 0.9793\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0741 - val_acc: 0.9823\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0880 - val_acc: 0.9804\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0133 - acc: 0.9956 - val_loss: 0.0937 - val_acc: 0.9803\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0109 - acc: 0.9962 - val_loss: 0.0964 - val_acc: 0.9801\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0851 - val_acc: 0.9810\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0848 - val_acc: 0.9836\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0989 - val_acc: 0.9796\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0076 - acc: 0.9975 - val_loss: 0.1087 - val_acc: 0.9804\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0081 - acc: 0.9977 - val_loss: 0.0992 - val_acc: 0.9799\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0962 - val_acc: 0.9814\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s - loss: 0.0084 - acc: 0.9974 - val_loss: 0.0899 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf9fa50320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### model structure (continued)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_cat, batch_size = batch_size, epochs = epoch,\n",
    "          verbose = 1,\n",
    "          validation_data = (x_test, y_test_cat)) ## validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss on Testing set: 0.0899214999654\n",
      "Accuracyt of Testing set: 0.9825\n"
     ]
    }
   ],
   "source": [
    "## model Save & load \n",
    "model.save('./DLdata/dnn_ver1.h5') ## save model\n",
    "model = load_model('./DLdata/dnn_ver1.h5') ## load model\n",
    "         \n",
    "          \n",
    "## case1          \n",
    "score = model.evaluate(x_test,y_test_cat,verbose=0)          \n",
    "print('Total loss on Testing set:', score[0])\n",
    "print('Accuracyt of Testing set:', score[1])\n",
    "        \n",
    "## cacs2\n",
    "pred = model.predict(x_test)\n",
    "classify = []\n",
    "for i in range(0,pred.shape[0]):\n",
    "    classify.append(np.argmax(pred[i]))\n",
    "classify =  np.asarray(classify)\n",
    "\n",
    "result = [y_test,classify] ## fin table\n",
    "result = np.matrix(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option : Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = 'weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5 ### checkpoint neural network model improvements\n",
    "\n",
    "filepath = 'weight-best.hdf5' ## check point best NN model Only \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
    "callbacks_list = [checkpoint]           \n",
    "\n",
    "model.fit(x_train,y_train_cat, validation_split=0.2, epochs = epoch, batch_size = batch_size,\n",
    "          callbacks = callbacks_list, verbose = 1 )\n",
    "\n",
    "## load weights\n",
    "model.load_weights(\"weight)\n",
    "\n",
    "############ parameters .. \n",
    "\n",
    "mode type : max // auto // min \n",
    "\n",
    "if monitor = val_acc then mode = max\n",
    "if monitor = val_loss then mode = min\n",
    "if mode = auto then automatically inferred from the nmae of the monitored quantity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option : Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', patience = 2, verbose = 2, mode = 'auto')\n",
    "callbacks_es = [earlystop]\n",
    "\n",
    "model.fit(x_train,y_train_cat, validation_split=0.2, epochs = epoch, batch_size = batch_size,\n",
    "          callbacks = callbacks_es, verbose = 1 )\n",
    "\n",
    "#### patience \n",
    "when the loss on validation set doesn't improve for 2 epochs  ==> stop training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option : Learning rate & weight decay & momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### learning rate & weight decay & momentum\n",
    "from keras.optimizers import *\n",
    "\n",
    "optim1 = SGD(lr = 0.01, momentum = 0.0, decay = 0.0)\n",
    "optim2 = Adagrad(lr = 0.01, decay = 0.0)\n",
    "optim3 = Adam(lr = 0.0001, decay = 0.0)\n",
    "....\n",
    "## reference site \n",
    "## http://keras.io/optimizers/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option : Dropout Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## dropout example\n",
    "#### deep neural network modeling  with dropout\n",
    "#### 데이터가 많거나 layer가 많을때, overfitting 방지 측면 \n",
    "#### 일반적으로는 activation function 위에 위치 시킴 (dense > dropout > activation > dense > dropout > activation)\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "info_input = Input(shape=(28*28,))\n",
    "\n",
    "layer0 = Dropout(0.5)(info_input)\n",
    "layer1 = Dense(512)(layer0)\n",
    "layer1_act = Activation('relu')(layer1)\n",
    "\n",
    "layer2 = Dropout(0.5)(layer1_act)\n",
    "\n",
    "layer3 = Dense(512)(layer2)\n",
    "layer3_act = Activation('relu')(layer3)\n",
    "\n",
    "layer4 = Dense(10)(layer3_act)\n",
    "layer4_act = Activation('softmax')(layer4)\n",
    "\n",
    "model = Model(inputs=[info_input], \n",
    "              outputs = [layer4_act])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = optim1,\n",
    "              metrics = ['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
