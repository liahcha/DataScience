{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuter newswire topics classification\n",
    "\n",
    "* 11,228 newswires from reuters\n",
    "* Lables : 46 topics\n",
    "* File : practice_Ver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "y_target :  [(3, 3972), (4, 2423), (19, 682), (16, 543), (1, 537), (11, 473), (20, 339), (13, 209), (8, 177), (10, 154), (21, 127), (9, 126), (25, 123), (2, 94), (18, 86), (24, 81), (0, 67), (6, 62), (12, 62), (36, 60), (28, 58), (30, 57), (34, 57), (23, 53), (31, 52), (17, 51), (40, 46), (32, 42), (41, 38), (26, 32), (15, 29), (39, 29), (14, 28), (43, 27), (29, 23), (38, 22), (22, 22), (5, 22), (37, 21), (7, 19), (45, 19), (27, 19), (44, 17), (35, 16), (42, 16), (33, 16)]\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n",
      "8982 train samples\n",
      "2246 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 46)                23598     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 798,766\n",
      "Trainable params: 798,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 0s - loss: 1.8113 - acc: 0.5731 - val_loss: 1.3808 - val_acc: 0.7197\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 0s - loss: 1.3719 - acc: 0.6766 - val_loss: 1.2341 - val_acc: 0.7430\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 0s - loss: 1.2103 - acc: 0.7130 - val_loss: 1.1706 - val_acc: 0.7631\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 0s - loss: 1.1029 - acc: 0.7297 - val_loss: 1.0871 - val_acc: 0.7620\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 0s - loss: 1.0231 - acc: 0.7469 - val_loss: 1.0559 - val_acc: 0.7686\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 0s - loss: 0.9715 - acc: 0.7555 - val_loss: 1.0342 - val_acc: 0.7720\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 0s - loss: 0.9260 - acc: 0.7642 - val_loss: 1.0031 - val_acc: 0.7798\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 0s - loss: 0.8779 - acc: 0.7735 - val_loss: 0.9787 - val_acc: 0.7853\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 0s - loss: 0.8382 - acc: 0.7866 - val_loss: 0.9669 - val_acc: 0.7987\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 0s - loss: 0.8031 - acc: 0.7898 - val_loss: 0.9629 - val_acc: 0.7887\n",
      "1536/2246 [===================>..........] - ETA: 0sTest score: 0.909276563147\n",
      "Test accuracy: 0.790739091772\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Dropout\n",
    "\n",
    "## parameter \n",
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "## load data \n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "## -----------------------\n",
    "## word index check\n",
    "word2idx = reuters.get_word_index()\n",
    "idx2word = dict([(v, k) for k, v in word2idx.items()]) ## check spyder explorer\n",
    "\n",
    "## y(target) index check\n",
    "from collections import Counter\n",
    "count = Counter()\n",
    "count.update(y_train)\n",
    "count.update(y_test)\n",
    "count.most_common()  # topic, data index\n",
    "print('y_target : ', count.most_common())\n",
    "## -----------------------\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# -------------------------------------------\n",
    "# train ( x_train / y_train )\n",
    "# test ( x_test / y_test )\n",
    "\n",
    "# hidden layers (dense 512, 256 , 128 , ... ) \n",
    "# activation function : relu for each layer \n",
    "# the activation function of last layer = softmax ? sigmoid ?\n",
    "\n",
    "info_input = Input(shape=(1000,))\n",
    "\n",
    "layer0 = Dropout(0.5)(info_input)\n",
    "layer1 = Dense(512)(layer0)\n",
    "layer1_act = Activation('relu')(layer1)\n",
    "\n",
    "layer2 = Dropout(0.5)(layer1_act)\n",
    "\n",
    "layer3 = Dense(512)(layer2)\n",
    "layer3_act = Activation('relu')(layer3)\n",
    "\n",
    "layer4 = Dropout(0.5)(layer3_act)\n",
    "\n",
    "layer5 = Dense(46)(layer4)\n",
    "layer5_act = Activation('softmax')(layer5)\n",
    "\n",
    "model = Model(inputs=[info_input], outputs = [layer5_act])\n",
    "\n",
    "### model structure\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs,\n",
    "          verbose = 1, \n",
    "          validation_split = 0.1) ## validation loss\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
