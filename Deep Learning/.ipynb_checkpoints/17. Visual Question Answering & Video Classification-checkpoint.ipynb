{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Image Captioning\n",
    "> Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3156-3164).\n",
    "\n",
    "![](./img/17_img_cap_03.JPG)\n",
    "\n",
    "- 주어진 사진을 자연어(text)로 묘사할 수 있는 모델 \n",
    "- 이미지 학습을 위한 CNN과 텍스트 생성을 위한 RNN 이용\n",
    "\n",
    "![](./img/17_img_cap.JPG)\n",
    "![](./img/17_img_cap_02.JPG)\n",
    "\n",
    "## Visual Question Answering (VQA) 모델\n",
    "\n",
    "- 주어진 질의를 사진에 기반하여 답변하는 모델\n",
    "  - 답변은 단답형 (단어)\n",
    "  - X: 이미지, 텍스트 (자연어)\n",
    "  - Y: 텍스트 (클래스) <br/><br/>\n",
    "  \n",
    "- 성능 평가 : http://cloudcv.org/vqa/\n",
    "![](./img/17_clodcv.png)\n",
    "\n",
    "\n",
    "## VQA 모델 구성\n",
    "\n",
    "- 이형의 데이터 (이미지, 텍스트)가 입력인 경우 multimodel 구조라 함\n",
    "- 이미지 학습 : CNN (VGGNet-19)\n",
    "- 텍스트 학습 : RNN (LSTM) + **word2vec**\n",
    "  * word2vec 를 LSTM의 input으로 활용 (embedding vector의 성능이 중요) <br/><br/>\n",
    "\n",
    "![](./img/17_vqa_model.JPG)\n",
    "\n",
    "\n",
    "## Multimodal Neural Network\n",
    "\n",
    "- 입력 X가 이형의 데이터로 구성된 경우\n",
    "![](./img/17_mnn.JPG)\n",
    "\n",
    "\n",
    "## Word Embedding (Word2Vec)\n",
    "\n",
    "- 임의의 단어를 주변 단어로 예측하는 신경망 모델 구성\n",
    "  - Hidden Layer의 unit 을 단어로 표현하는 저차원의 새로운 변수로 해석\n",
    "- 기존 : One-hot vector\n",
    "- 신규 : Word2Vec \n",
    "  * 사람들이 사용하는 글의 패턴을 이용해서 학습을 하는 방식 (label 필요 없고, 대량의 text만 있으면 됨)\n",
    "  * **단어간의 유사도 파악 가능**\n",
    "![](./img/17_word2vec_01.JPG)\n",
    "\n",
    "- 모델 구성 방식에 따라 \n",
    "  - CBOW (더 선호되는 방식)\n",
    "  - Skip-gram\n",
    "![](./img/17_cbow.JPG)\n",
    "\n",
    "\n",
    "### Word2Vec : 단어간 연산\n",
    "\n",
    "- Word2Vec 으로 얻은 단어 벡터를 이용하여 단어간 연산 수행 가능\n",
    "- 저차원 벡터가 단어의 의미를 반영하여 학습되었음을 의미\n",
    "- $vec(\"man\") - vec(\"king\") + vec(\"women\") = vec(\"queen\")$\n",
    "\n",
    "![](./img/17_word2vec.JPG)\n",
    "\n",
    "\n",
    "## Video Classification\n",
    "\n",
    "- Activity Recognition\n",
    "- 영상은 프레임 분할하여, 프레임을 단일 이미지로 이해 : **CNN + RNN 모델**\n",
    "  - 인접변수간 높은 상관관계를 갖는 이미지의 특성 : CNN\n",
    "  - 입력변수간 순서가 있는 특성 : RNN\n",
    "  \n",
    "#### CNN + RNN 모델\n",
    "- CNN과 RNN을 연결하는 방식에 따라 많은 변형이 존재 \n",
    "- 실험 결과 1) Convolution 방법의 성능이 가장 좋았음\n",
    "\n",
    "![](./img/17_cnn_rnn.JPG)\n",
    "\n",
    "### 1) 2D Convolution layer 이용 \n",
    "- 영상의 프레임을 채널로 간주, 2차원 convolution layer를 이용한 모델링\n",
    "- 시간에 따른 프레임 변화를 고려할 수 없음\n",
    "\n",
    "### 2) 3D Convolution layer 이용\n",
    "\n",
    "> Ji, S., Xu, W., Yang, M., & Yu, K. (2013). 3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1), 221-231 <br/>\n",
    "> - https://ai2-s2-pdfs.s3.amazonaws.com/3c86/dfdbdf37060d5adcff6c4d7d453ea5a8b08f.pdf\n",
    "\n",
    "- 영상의 프레임을 채널로 간주, 3차원 convolution layuer를 이용한 모델링\n",
    "  - keras에서는 Conv3D 로 구현되어 있음\n",
    "- 인접 시점 간 프레임의 상관성을 3D Conv.를 통해 고려\n",
    "- 길이가 다른 영상을 고려할 수 없음\n",
    "\n",
    "![](./img/17_3d_conv.png)\n",
    "\n",
    "### 3) CNN + LSTM 방법\n",
    "\n",
    "> Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., & Darrell, T. (2015). Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp.2625-2634) <br/>\n",
    "> - https://arxiv.org/abs/1411.4389\n",
    "\n",
    "- 단일 프레임에서 CNN으로 변수를 추출하고 이를 LSTM에 전달\n",
    "- 서로 다른 길이 영상에 대한 모델링 가능\n",
    "\n",
    "![](./img/17_CNN_LSTM.png)\n",
    "\n",
    "\n",
    "### 4) Conv. LSTM \n",
    "\n",
    "> Xingjian, S. H. I., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K., & Woo, W. C. (2015). Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems (pp. 802-810). <br/>\n",
    "> - https://arxiv.org/abs/1506.04214\n",
    "\n",
    "- LSTM 내부의 $C_{t}$, $h_{t}$ 을 입력 데이터와 동일하게 convolution 연산을 수행\n",
    "- 서로 다른 길이의 영상에 대해서 모델링 가능\n",
    "\n",
    "![](./img/17_ConvLSTM.png)\n",
    "\n",
    "정보는 feature map의 형태로 구성\n",
    "\n",
    "## LSTM \n",
    "\n",
    "- 기존 LSTM vs. ConvLSTM\n",
    "\n",
    "![](./img/17_conv_lstm.JPG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
