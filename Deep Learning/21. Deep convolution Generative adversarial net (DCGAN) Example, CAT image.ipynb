{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep convolution Generative adversarial net (DCGAN) for CAT image\n",
    "\n",
    "- 상대적으로 안정적인(Stable) GAN을 만들고자 함\n",
    "  - 어떠한 형태의 이미지가 들어오더라도, \n",
    "- DCGAN으로 MNIST 데이터 생성\n",
    "\n",
    "![](./img/21_DCGAN_1.png)\n",
    "\n",
    "- SKT Brain, DISCO GAN : https://github.com/SKTBrain\n",
    "\n",
    "### Generative model\n",
    "\n",
    "|Layer (type)           |      Output Shape     |         Param #   | memo | \n",
    "|:--------------------|:--------------------|:--------------------|:-----|\n",
    "|input_1 (InputLayer)       |  (None, 100)        |       0         |  | \n",
    "|dense_1 (Dense)             | (None, 8192)       |       827392    |  |\n",
    "|leaky_re_lu_1 (LeakyReLU)    |(None, 8192)       |       0         |  |\n",
    "|batch_normalization_1 (BatchNormalization)| (None, 8192)       |       32768     |  |\n",
    "|reshape_1 (Reshape)         | (None, 4, 4, 512)  |       0         |  |\n",
    "|conv2d_1 (Conv2D)           | (None, 4, 4, 512)  |       2359808   |  |\n",
    "|leaky_re_lu_2 (LeakyReLU)   | (None, 4, 4, 512)  |       0         |  |\n",
    "|batch_normalization_2 (BatchNormalization) | (None, 4, 4, 512)  |       2048      |  |\n",
    "|up_sampling2d_1 (UpSampling2)| (None, 8, 8, 512)  |       0         | 4 by 4 짜리를 over sampling (2배)  |\n",
    "|conv2d_2 (Conv2D)           | (None, 8, 8, 256)  |       1179904   |  |\n",
    "|leaky_re_lu_3 (LeakyReLU)   | (None, 8, 8, 256)  |       0         |  |\n",
    "|batch_normalization_3 (BatchNormalization)| (None, 8, 8, 256)  |       1024      |  |\n",
    "|up_sampling2d_2 (UpSampling2)| (None, 16, 16, 256)|       0         |  |\n",
    "|conv2d_3 (Conv2D)           | (None, 16, 16, 128)|       524416    |  |\n",
    "|leaky_re_lu_4 (LeakyReLU)   | (None, 16, 16, 128)|       0         |  |\n",
    "|batch_normalization_4 (BatchNormalization)| (None, 16, 16, 128) |      512       |  |\n",
    "|up_sampling2d_3 (UpSampling2)| (None, 32, 32, 128) |      0         |  |\n",
    "|conv2d_4 (Conv2D)           | (None, 32, 32, 64)  |      204864    |  |\n",
    "|leaky_re_lu_5 (LeakyReLU)   | (None, 32, 32, 64)  |      0         |  |\n",
    "|batch_normalization_5 (BatchNormalization)| (None, 32, 32, 64)  |      256       |  |\n",
    "|up_sampling2d_4 (UpSampling2)| (None, 64, 64, 64)  |      0         |  |\n",
    "|conv2d_5 (Conv2D)           | (None, 64, 64, 3)   |      4803      |  |\n",
    "|activation_1 (Activation)   | (None, 64, 64, 3)   |      0         |  |\n",
    "\n",
    "- Total params: 5,137,795\n",
    "- Trainable params: 5,119,491\n",
    "- Non-trainable params: 18,304\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,random\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.activations import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import backend as K\n",
    "K.image_data_format()\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "\n",
    "def plot_loss(losses):\n",
    "#        display.clear_output(wait=True)\n",
    "#        display.display(plt.gcf())\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(losses[\"d\"], label='discriminitive loss')\n",
    "        plt.plot(losses[\"g\"], label='generative loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_gen(n_ex=25,dim=(5,5), figsize=(5,5)):\n",
    "    noise = np.random.normal(0,1,size=[n_ex,100])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = ((generated_images * 127.5) + 127.5).astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0],dim[1],i+1)\n",
    "        img = generated_images[i,:,:,:]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_real(n_ex=25,dim=(5,5), figsize=(5,5) ):\n",
    "\n",
    "    idx = np.random.randint(0,X_train.shape[0],n_ex)\n",
    "    generated_images = X_train[idx,:,:,:]\n",
    "    generated_images = ((generated_images * 127.5) + 127.5).astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0],dim[1],i+1)\n",
    "        img = generated_images[i,:,:,:]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## load data\n",
    "filedir = path+'/datasets/cats64/'\n",
    "filenames = os.listdir(filedir)\n",
    "\n",
    "X_train = []\n",
    "for filename in filenames:\n",
    "    img = imread(os.path.join(filedir, filename))\n",
    "    X_train.append(img)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "print(np.min(X_train), np.max(X_train))\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "\n",
    "\n",
    "\n",
    "shp = X_train.shape[1:]\n",
    "opt = Adam(lr = 0.0002, beta_1 = .5, clipnorm=1.)\n",
    "dopt = Adam(lr = 0.0001, beta_1 = .5, clipnorm=1.)\n",
    "\n",
    "\n",
    "# Build Generative model ...\n",
    "g_input = Input(shape=[100])\n",
    "H = Dense(512*4*4,kernel_initializer='glorot_normal')(g_input)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = BatchNormalization()(H)\n",
    "\n",
    "H = Reshape([4,4,512])(H) # dimension setting\n",
    "\n",
    "H = Conv2D(512,(3,3),padding = 'same', kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = BatchNormalization()(H)\n",
    "\n",
    "H = UpSampling2D()(H)\n",
    "H = Conv2D(256,(3,3),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = BatchNormalization()(H)\n",
    "\n",
    "H = UpSampling2D()(H)\n",
    "H = Conv2D(128,(4,4),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = BatchNormalization()(H)\n",
    "\n",
    "H = UpSampling2D()(H)\n",
    "H = Conv2D(64,(5,5),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = BatchNormalization()(H)\n",
    "\n",
    "H = UpSampling2D()(H)\n",
    "H = Conv2D(3,(5,5),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "g_V = Activation('tanh')(H)\n",
    "\n",
    "generator = Model(inputs = g_input, outputs = g_V)\n",
    "generator.compile(loss = 'binary_crossentropy', optimizer = opt)\n",
    "generator.summary()\n",
    "\n",
    "# Build Discriminative model ...\n",
    "d_input = Input(shape=shp)\n",
    "\n",
    "H = Conv2D(64,(4,4),strides=(2,2),padding='same',kernel_initializer='glorot_normal')(d_input)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "\n",
    "H = Conv2D(128,(4,4),strides=(2,2),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(0.5)(H)\n",
    "\n",
    "H = Conv2D(256,(4,4),strides=(2,2),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(0.5)(H)\n",
    "\n",
    "H = Conv2D(512,(4,4),strides=(2,2),padding='same',kernel_initializer='glorot_normal')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(0.5)(H)\n",
    "\n",
    "H = Flatten()(H) # for 1 classification\n",
    "d_V = Dense(1, activation = 'sigmoid')(H)\n",
    "\n",
    "discriminator = Model(d_input,d_V)\n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer=dopt)\n",
    "\n",
    "## combine model\n",
    "discriminator.trainable = False\n",
    "GAN_input= Input(shape = (100,))\n",
    "gen_sample = generator(GAN_input)\n",
    "GAN_output = discriminator(gen_sample)\n",
    "\n",
    "GAN = Model(inputs = GAN_input, outputs = GAN_output)\n",
    "GAN.compile(loss = 'binary_crossentropy', optimizer= opt)\n",
    "\n",
    "##############\n",
    "## train GAN model\n",
    "\n",
    "#epoch = 10000\n",
    "epoch = 10\n",
    "batch_size = 32\n",
    "freq = 1\n",
    "\n",
    "batch_count = X_train.shape[0]//batch_size\n",
    "\n",
    "losses = {\"d\":[], \"g\":[]}\n",
    "for i in range(epoch):\n",
    "    for j in tqdm(range(batch_count)):\n",
    "        noise_input = np.random.normal(0, 1, size= (batch_size, 100))\n",
    "\n",
    "        real_image = X_train[np.random.randint(0, X_train.shape[0],\n",
    "                                                size = batch_size)]\n",
    "\n",
    "        fake_image = generator.predict(noise_input, batch_size=batch_size)\n",
    "\n",
    "        X = np.concatenate([fake_image,real_image])\n",
    "        y_discriminator = [0]*batch_size + [1] * batch_size\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(X, y_discriminator)\n",
    "        losses[\"d\"].append(d_loss)\n",
    "\n",
    "\n",
    "        noise_input = np.random.normal(0, 1, size= (batch_size, 100))\n",
    "        y_generator = [1] * batch_size\n",
    "        discriminator.trainable = False\n",
    "        g_loss = GAN.train_on_batch(noise_input, y_generator)\n",
    "        losses[\"g\"].append(g_loss)\n",
    "\n",
    "    if i%freq==freq-1:\n",
    "       plot_loss(losses)\n",
    "       plot_gen()\n",
    "\n",
    "    if i%10==10-1:\n",
    "       print(\"###############################################\")\n",
    "       print(\"iteration = \", i+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot_gen()\n",
    "#plot_real()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "after # iteration =  710\n",
    "\n",
    "![](./img/21_gen_cat_1.png)\n",
    "![](./img/21_gen_cat_2.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
