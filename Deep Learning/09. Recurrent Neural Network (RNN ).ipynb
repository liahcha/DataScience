{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RNN\n",
    "\n",
    "- 변수간 순서가 있는 경우 유용\n",
    "  * 그림의 화살표는 파라미터라고 이해하면 됨\n",
    "  * 모든 모델은 동일 (반복되는 구조)\n",
    "\n",
    "\n",
    "## RNN이 사용되는 텍스트 \"감성분석\" 예제\n",
    "\n",
    "- 문장은 단어로 구성되며 단어의 순서가 주요\n",
    "- 감성분석 : 긍정/부정 classification 모델\n",
    "\n",
    "- 기존에는 document term matrix를 구성하여 분류 모델 구축\n",
    "  - 문장의 순서를 고려하지 못함 \n",
    "  \n",
    "- 순서를 고려한 모델링 $\\rightarrow$ 모델의 파라미터가 너무 많아짐\n",
    "  - 서로 다른 길이의 문장을 고려하지 못함\n",
    "\n",
    "- **RNN 적용**\n",
    "  - 순서를 고려할 수 있는 RNN 모델 적용 시, 현재의 정보를 미래에 반영 가능\n",
    "  - 각 단어를 one-hot vector로 표기하여 input으로 넣어줌 (예: '나는' = 1,0,0,0,0,0)\n",
    "  \n",
    "  - 실제로는 dense-layer의 형태로 되어 있음\n",
    "\n",
    "\n",
    "## RNN 알고리즘\n",
    "\n",
    "- Hidden \n",
    "  - $S_{t} = tanh(U_{x_{t}} + W_{S_{t-1}}$\n",
    "\n",
    "- Loss function\n",
    "  - cross entropy (classification probroem)\n",
    "\n",
    "## Back Propagation in RNN\n",
    "\n",
    "- Back Propagation Through Time (BPTT) 알고리즘\n",
    "- RNN의 파라미터 U, V, W를 업데이트\n",
    "\n",
    "1) $\\frac{\\partial L_{t}}{\\partial V} = $ <br/>\n",
    "2) $1$ <br/>\n",
    "3) $1$ <br/>\n",
    "\n",
    "\n",
    "## BPTT 예제\n",
    "\n",
    "- 전달 과정의 gradient 를 모두 sum 해줌\n",
    "- 마지막 결과의 output을 최종 classficiation 결과로 가져가게 됨\n",
    "\n",
    "\n",
    "## RNN 의 한계\n",
    "\n",
    "- 길이가 긴 sequence의 경우, exploding gradient / vanishing gradient 문제가 발생하기 쉬움 (layer를 많이 쌓지 않아도)\n",
    "- Exploding gradient : clip gradient\n",
    "- Vanishing gradient problem 을 극복하기 위해 : 다른 모델 사용 (LSTM, GRU)\n",
    "\n",
    "\n",
    "## 1. LSTM\n",
    "\n",
    "- Long Short-Term Memory\n",
    "- Gate(스위치) 개념을 추가하여 길이가 가니 sequence 를 모델링\n",
    "- 다양한 변형 모델이 존재\n",
    " \n",
    "\n",
    "## 2. LSTM 의 단계\n",
    "\n",
    "### 1) $f_{c}$ : forget gate\n",
    "\n",
    "### 2) $i_{t}$ : input gate\n",
    "Ct (정보량) 결정 됨\n",
    "\n",
    "### 3) Update $C_{t}$ : cell state (정보량)\n",
    "- 다음셀로 보낼 정보량 \n",
    "\n",
    "### 4) $o_{t}$ & $h_{t}$ : output gate\n",
    "\n",
    "\n",
    "## 3. GRU\n",
    "\n",
    "- GRU : Gated recurrent unit 은 LSTM의 변형 버전\n",
    "- $C_{t}$ 를 업데이트 할 때, forget gate만으로 구성함 \n",
    "\n",
    "\n",
    "## RNN을 이용한 다양한 모델링\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
