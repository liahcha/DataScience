{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning, on Atari Breakout\n",
    "\n",
    "> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level\n",
    "control through deep reinforcement learning. Nature , 518 (7540), 529-533. \n",
    "\n",
    "- **Deep Q-Network (DQN)**\n",
    "\n",
    "![](./img/24_dqn_atari_breakout.png)\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "![](./img/24_dqn_algorithm.png)\n",
    "\n",
    "\n",
    "- Reference : https://rllab.readthedocs.io/en/latest/user/gym_integration.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. install dependencies\n",
    "\n",
    "> sudo apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev libboost-all-dev libsdl2-dev swig\n",
    "\n",
    "#### B. install AiGym and Atari environment\n",
    "\n",
    "#### Method 1\n",
    "\n",
    "> pip install gym <br/>\n",
    "> pip install atari-py\n",
    "\n",
    "#### Method 2\n",
    "> pip install gym[atari]\n",
    "\n",
    "#### C. install AiGym and all supported game environments\n",
    ">pip install gym[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 00:37:20,494] Making new env: BreakoutDeterministic-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,685,667\n",
      "Trainable params: 1,685,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,685,667\n",
      "Trainable params: 1,685,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Summary name Total Reward/Episode is illegal; using Total_Reward/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 00:37:20,881] Summary name Total Reward/Episode is illegal; using Total_Reward/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Average Max Q/Episode is illegal; using Average_Max_Q/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 00:37:20,882] Summary name Average Max Q/Episode is illegal; using Average_Max_Q/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Average Loss/Episode is illegal; using Average_Loss/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 00:37:20,885] Summary name Average Loss/Episode is illegal; using Average_Loss/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " episode: 0  score: 10.0  memory_length: 449  epsilon: 0.3  global_step: 449  average_q: 3.57314498148  average_loss: 0.0\n",
      " episode: 1  score: 21.0  memory_length: 1203  epsilon: 0.3  global_step: 1203  average_q: 3.8268975957  average_loss: 0.0\n",
      " episode: 2  score: 13.0  memory_length: 1819  epsilon: 0.3  global_step: 1819  average_q: 3.69383783213  average_loss: 0.0\n",
      " episode: 3  score: 26.0  memory_length: 2778  epsilon: 0.3  global_step: 2778  average_q: 4.06954123653  average_loss: 0.0\n",
      " episode: 4  score: 30.0  memory_length: 3774  epsilon: 0.3  global_step: 3774  average_q: 4.42357091564  average_loss: 0.0\n",
      " episode: 5  score: 23.0  memory_length: 4819  epsilon: 0.3  global_step: 4819  average_q: 3.7885015466  average_loss: 0.0\n",
      "Updating target model!\n",
      " episode: 6  score: 22.0  memory_length: 5621  epsilon: 0.3  global_step: 5621  average_q: 3.72736808903  average_loss: 0.0\n",
      " episode: 7  score: 6.0  memory_length: 5912  epsilon: 0.3  global_step: 5912  average_q: 3.59518504102  average_loss: 0.0\n",
      " episode: 8  score: 10.0  memory_length: 6333  epsilon: 0.3  global_step: 6333  average_q: 3.72650568078  average_loss: 0.0\n",
      " episode: 9  score: 20.0  memory_length: 7205  epsilon: 0.3  global_step: 7205  average_q: 3.70758202727  average_loss: 0.0\n",
      " episode: 10  score: 19.0  memory_length: 7910  epsilon: 0.3  global_step: 7910  average_q: 4.42508079972  average_loss: 0.0\n",
      " episode: 11  score: 13.0  memory_length: 8473  epsilon: 0.3  global_step: 8473  average_q: 3.65949262141  average_loss: 0.0\n",
      " episode: 12  score: 24.0  memory_length: 9394  epsilon: 0.3  global_step: 9394  average_q: 3.84737402686  average_loss: 0.0\n",
      "Updating target model!\n",
      " episode: 13  score: 18.0  memory_length: 10103  epsilon: 0.2792000000000023  global_step: 10103  average_q: 3.80497465551  average_loss: 0.00266695082958\n",
      " episode: 14  score: 27.0  memory_length: 11056  epsilon: 0.0998000000000012  global_step: 11056  average_q: 3.84433542406  average_loss: 0.0200945303876\n",
      " episode: 15  score: 51.0  memory_length: 12515  epsilon: 0.0998000000000012  global_step: 12515  average_q: 4.96024393512  average_loss: 0.0142324962711\n",
      " episode: 16  score: 46.0  memory_length: 13854  epsilon: 0.0998000000000012  global_step: 13854  average_q: 4.7945823316  average_loss: 0.0164429213901\n",
      " episode: 17  score: 28.0  memory_length: 14865  epsilon: 0.0998000000000012  global_step: 14865  average_q: 3.88658594059  average_loss: 0.0164883323744\n",
      "Updating target model!\n",
      " episode: 18  score: 51.0  memory_length: 16181  epsilon: 0.0998000000000012  global_step: 16181  average_q: 4.59743537421  average_loss: 0.0165996779154\n",
      " episode: 19  score: 35.0  memory_length: 17383  epsilon: 0.0998000000000012  global_step: 17383  average_q: 4.25587426674  average_loss: 0.015124237052\n",
      " episode: 20  score: 18.0  memory_length: 18113  epsilon: 0.0998000000000012  global_step: 18113  average_q: 3.66723942626  average_loss: 0.0152260957802\n",
      " episode: 21  score: 41.0  memory_length: 19497  epsilon: 0.0998000000000012  global_step: 19497  average_q: 5.19664303054  average_loss: 0.0158012360974\n",
      "Updating target model!\n",
      " episode: 22  score: 32.0  memory_length: 20693  epsilon: 0.0998000000000012  global_step: 20693  average_q: 4.79890650411  average_loss: 0.0156952056032\n",
      " episode: 23  score: 39.0  memory_length: 21999  epsilon: 0.0998000000000012  global_step: 21999  average_q: 3.93923117665  average_loss: 0.0137959371293\n",
      " episode: 24  score: 53.0  memory_length: 23530  epsilon: 0.0998000000000012  global_step: 23530  average_q: 4.38866979222  average_loss: 0.0139438675979\n",
      "Updating target model!\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Number of episodes to run\n",
    "# 50000 episodes take over 1 week on single GPU\n",
    "EPISODES = 50000\n",
    "\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "# Define DQN Agent\n",
    "class DQNAgent(object):\n",
    "    \"\"\"DQN Agent class\"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Configurations\n",
    "        self.render = True # if false = 게임하는 화면이 뜨지 않음\n",
    "        self.load_model = True\n",
    "        # Environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # Epsilon parameters\n",
    "        self.epsilon = 0.3 \t\t\t         # original paper; 1.0\n",
    "        self.epsilon_start = 0.3 \t\t      # original paper; 1.0\n",
    "        self.epsilon_end = 0.1\n",
    "        self.exploration_steps = 1000. \t\t# original paper; 1000000\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                   / self.exploration_steps\n",
    "        # Training parameters\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 10000 \t\t      # original paper; 50000\n",
    "        self.update_target_rate = 5000 \t\t# original paper; 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # Build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimize = self.optimize()\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max = 0.\n",
    "        self.avg_loss = 0.\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(logdir='summary/breakout_dqn',\n",
    "                                                    graph=self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(path+'/save_model/breakout_dqn.h5')\n",
    "\n",
    "    # If the error is in [-1, 1], then the cost is quadratic to the error\n",
    "    # But if it is outside the interval, the cost is linear to the error\n",
    "    def optimize(self):\n",
    "        # a denotes action, y denotes prediction of model\n",
    "        a = K.placeholder(shape=(None, ), dtype='int32')\n",
    "        y = K.placeholder(shape=(None, ), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # Approximate Q function using CNN\n",
    "    # Input: state\n",
    "    # Output: action\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Approximate Q function using CNN.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32, kernel_size=[8, 8], strides=[4, 4], activation='relu',\n",
    "                         input_shape=self.state_size))\n",
    "        model.add(Conv2D(filters=64, kernel_size=[4, 4], strides=[2, 2], activation='relu'))\n",
    "        model.add(Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # After some time interval, update the target model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # Save sample <s,a,r,s'> to the replay memory\n",
    "    def save_to_replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append([history, action, reward, next_history, dead])\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_with_replay_memory(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        # From replay memory, sample a mini-batch\n",
    "        # one unit in memory = [history, action, reward, next_history, dead]\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        # Define shape of history & next history\n",
    "        history_shape = (self.batch_size, ) + self.state_size\n",
    "        history = np.zeros(shape=history_shape)\n",
    "        next_history = np.zeros(shape=history_shape)\n",
    "\n",
    "        target = np.zeros((self.batch_size, ))\n",
    "\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        # iteration in minibatch\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        # Like Q-learning, set maximum Q-value at s' as target value\n",
    "        # by predicting it with the target network! (model-free rl)\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i] + self.discount_factor * 0.\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimize([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "\n",
    "    # Make summary operators for Tensorboard\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(initial_value=0.)\n",
    "        episode_avg_max_q = tf.Variable(initial_value=0.)\n",
    "        episode_duration = tf.Variable(initial_value=0.)\n",
    "        episode_avg_loss = tf.Variable(initial_value=0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(dtype=tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        assert len(summary_vars) == len(summary_placeholders)\n",
    "\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "\n",
    "# 210 x 160 x 3 (color) --> 84 x 84 (gray)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "\n",
    "    scores, episodes, global_step = [], [], 0\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        # Do nothing at the start of episodes to avoid sub-optimal\n",
    "        # 1 means do nothing\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # At start of episodes, there is no preceding frame\n",
    "        # Just copy initial states to make history\n",
    "        assert observe.shape == (210, 160, 3)\n",
    "\n",
    "        state = pre_processing(observe)\n",
    "        assert state.shape == (84, 84)\n",
    "\n",
    "        # inital statues shape 맞추기 위해서 [state, state, state, state]\n",
    "        history = np.stack([state, state, state, state], axis=2)\n",
    "        assert history.shape == (84, 84, 4)\n",
    "\n",
    "        history = np.reshape(history, (1, 84, 84, 4))\n",
    "        assert history.shape == (1, 84, 84, 4) # 1 : batch dimension\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # Get action for the current history and go one step in the environment\n",
    "            action = agent.get_action(history) # get_action 함수는 q 값에 해당하는 state를 return 함\n",
    "            # Change action to real action\n",
    "            if action == 0:\n",
    "                real_action = 1 \n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            elif action == 2:\n",
    "                real_action = 3\n",
    "\n",
    "            ## Get observe, reward, done, info - after action\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            assert observe.shape == (210, 160, 3)\n",
    "            # Preprocess the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            assert next_state.shape == (84, 84)\n",
    "\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            assert next_state.shape == (1, 84, 84, 1)\n",
    "\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "            # If the agent missed the ball, agent is dead. but episode is not over.\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead = True\n",
    "                start_life = info['ale.lives']\n",
    "\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            # Save the sample <s,a,r,s'> to the replay memory\n",
    "            agent.save_to_replay_memory(history, action, reward, next_history, dead)\n",
    "            # Train model\n",
    "            agent.train_with_replay_memory()\n",
    "            # Update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "                print('Updating target model!')\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # If the agent is dead, then reset the history\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            # If episode is done, plot the scores\n",
    "            if done:\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i],\n",
    "                                       feed_dict={agent.summary_placeholders[i]: float(stats[i])\n",
    "                                                  }\n",
    "                                       )\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(' episode:', e, ' score:', score, ' memory_length:', len(agent.memory),\n",
    "                      ' epsilon:', agent.epsilon, ' global_step:', global_step,\n",
    "                      ' average_q:', agent.avg_q_max / float(step), ' average_loss:', agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0., 0.\n",
    "\n",
    "        if (e + 1) % 100 == 0:\n",
    "            agent.model.save_weights(path+'/save_model/breakout_dqn_episode_{}.h5'.format(e + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/24_dqn_atari_b2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
