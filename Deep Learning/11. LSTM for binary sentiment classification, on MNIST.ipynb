{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for binary sentiment classification, on MNIST\n",
    "\n",
    "- Classify MNIST data with LSTM\n",
    "- Use 28 timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:80] 2017-07-06 00:59:54,449 > MNIST data has been loaded.\n",
      "[INFO:80] 2017-07-06 00:59:54,449 > MNIST data has been loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:101] 2017-07-06 00:59:54,534 > final training data shape: (60000, 28, 28)\n",
      "[INFO:101] 2017-07-06 00:59:54,534 > final training data shape: (60000, 28, 28)\n",
      "[INFO:102] 2017-07-06 00:59:54,535 > final test data shape: (10000, 28, 28)\n",
      "[INFO:102] 2017-07-06 00:59:54,535 > final test data shape: (10000, 28, 28)\n",
      "[INFO:109] 2017-07-06 00:59:54,537 > final train label shape: (60000,)\n",
      "[INFO:109] 2017-07-06 00:59:54,537 > final train label shape: (60000,)\n",
      "[INFO:110] 2017-07-06 00:59:54,538 > final test label shape: (10000,)\n",
      "[INFO:110] 2017-07-06 00:59:54,538 > final test label shape: (10000,)\n",
      "[INFO:190] 2017-07-06 00:59:54,657 > Using tensorboard callback\n",
      "[INFO:190] 2017-07-06 00:59:54,657 > Using tensorboard callback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sequence (InputLayer)  (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               51600     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 62,710\n",
      "Trainable params: 62,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "INFO:tensorflow:Summary name lstm_2/kernel:0 is illegal; using lstm_2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,457 > Summary name lstm_2/kernel:0 is illegal; using lstm_2/kernel_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,457 > Summary name lstm_2/kernel:0 is illegal; using lstm_2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm_2/recurrent_kernel:0 is illegal; using lstm_2/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,460 > Summary name lstm_2/recurrent_kernel:0 is illegal; using lstm_2/recurrent_kernel_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,460 > Summary name lstm_2/recurrent_kernel:0 is illegal; using lstm_2/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm_2/bias:0 is illegal; using lstm_2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,462 > Summary name lstm_2/bias:0 is illegal; using lstm_2/bias_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,462 > Summary name lstm_2/bias:0 is illegal; using lstm_2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,464 > Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,464 > Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,466 > Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,466 > Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,469 > Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,469 > Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 00:59:55,471 > Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n",
      "[INFO:82] 2017-07-06 00:59:55,471 > Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9580Epoch 00000: val_acc improved from -inf to 0.98442, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.00-0.98.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.1133 - acc: 0.9582 - val_loss: 0.0474 - val_acc: 0.9844\n",
      "Epoch 2/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9870Epoch 00001: val_acc improved from 0.98442 to 0.99248, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.01-0.99.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0376 - acc: 0.9870 - val_loss: 0.0222 - val_acc: 0.9925\n",
      "Epoch 3/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9918Epoch 00002: val_acc improved from 0.99248 to 0.99455, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.02-0.99.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0247 - acc: 0.9918 - val_loss: 0.0164 - val_acc: 0.9946\n",
      "Epoch 4/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9942Epoch 00003: val_acc improved from 0.99455 to 0.99528, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.03-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0179 - acc: 0.9942 - val_loss: 0.0134 - val_acc: 0.9953\n",
      "Epoch 5/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9952Epoch 00004: val_acc improved from 0.99528 to 0.99582, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.04-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0121 - val_acc: 0.9958\n",
      "Epoch 6/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9962Epoch 00005: val_acc improved from 0.99582 to 0.99598, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.05-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0115 - acc: 0.9962 - val_loss: 0.0139 - val_acc: 0.9960\n",
      "Epoch 7/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9968Epoch 00006: val_acc improved from 0.99598 to 0.99690, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.06-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0092 - val_acc: 0.9969\n",
      "Epoch 8/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9972Epoch 00007: val_acc improved from 0.99690 to 0.99717, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.07-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0093 - val_acc: 0.9972\n",
      "Epoch 9/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9976Epoch 00008: val_acc improved from 0.99717 to 0.99732, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.08-1.00.hdf5\n",
      "54000/54000 [==============================] - 10s - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0098 - val_acc: 0.9973\n",
      "Epoch 10/10\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9979- ETA: 0s - loss: 0.0063 - Epoch 00009: val_acc did not improve\n",
      "54000/54000 [==============================] - 10s - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0094 - val_acc: 0.9970\n",
      " 9856/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:239] 2017-07-06 01:01:42,610 > Test accuracy: 99.57%\n",
      "[INFO:239] 2017-07-06 01:01:42,610 > Test accuracy: 99.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sequence (InputLayer)  (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               51600     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 62,710\n",
      "Trainable params: 62,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9824/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:255] 2017-07-06 01:01:45,085 > Test accuracy: 99.66%\n",
      "[INFO:255] 2017-07-06 01:01:45,085 > Test accuracy: 99.66%\n"
     ]
    }
   ],
   "source": [
    "## Single LSTM Layer\n",
    "\n",
    "### Step 1: Import modules\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "## Fix random seed for reproducibility\n",
    "np.random.seed(20170704)\n",
    "\n",
    "\n",
    "## Check proper working directory\n",
    "#os.chdir('path/to/day_2/')\n",
    "#if os.getcwd().split('/')[-1] == 'day_2':\n",
    "#    pass\n",
    "#else:\n",
    "#    raise OSError('Check current working directory.\\n'\n",
    "#                  'If not specified as instructed, '\n",
    "#                  'more errors will occur throught the code.\\n'\n",
    "#                  '- Current working directory: %s' % os.getcwd())\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "\n",
    "## Set logging\n",
    "def set_logging(testlog=False):\n",
    "    # 1. Make 'logger' instance\n",
    "    logger = logging.getLogger()\n",
    "    # 2. Make 'formatter'\n",
    "    formatter = logging.Formatter(\n",
    "            '[%(levelname)s:%(lineno)s] %(asctime)s > %(message)s'\n",
    "            )\n",
    "    # 3. Make 'streamHandler'\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    # 4. Set 'formatter' to 'streamHandler'\n",
    "    streamHandler.setFormatter(formatter)\n",
    "    # 5. Add streamHandler to 'logger' instance\n",
    "    logger.addHandler(streamHandler)\n",
    "    # 6. Set level of log; DEBUG -> INFO -> WARNING -> ERROR -> CRITICAL\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # 7. Print test INFO message\n",
    "    if testlog: # default is 'False'\n",
    "        logging.info(\"Stream logging available!\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "_ = set_logging()\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 2: Load & preprocess data\n",
    "\n",
    "## 2-1. Load\n",
    "# Data, shuffled and split between train / test sets\n",
    "# shape of X_train, X_test; (batch_size, height, width)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "logging.info('MNIST data has been loaded.')\n",
    "\n",
    "## 2-2. Preprocess\n",
    "# Change data types to 'float32'\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "# Normalization\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Check input shape of data; (batch_size, timesteps, input_dim)\n",
    "timesteps = 28\n",
    "input_dim = 28\n",
    "#timesteps = 14\n",
    "#input_dim = 56\n",
    "\n",
    "X_train = X_train.reshape(-1, timesteps, input_dim) # -1: reshape 할때, 알아서 채우라는 의미\n",
    "X_test = X_test.reshape(-1, timesteps, input_dim)\n",
    "\n",
    "logging.info('final training data shape: {}'.format(X_train.shape))\n",
    "logging.info('final test data shape: {}'.format(X_test.shape))\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot vectors)\n",
    "num_classes = 10\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "logging.info('final train label shape: {}'.format(y_train.shape))\n",
    "logging.info('final test label shape: {}'.format(y_test.shape))\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 3: Build Model\n",
    "\n",
    "## 3-1. Define hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "\n",
    "## 3-2. Define RNN model with LSTM cells for MNIST data\n",
    "\n",
    "# TODO: DEFINE INPUT TENSOR (hint; (timesteps, input_dim))\n",
    "input_sequences = Input(shape=(timesteps, input_dim), name='input_sequence')\n",
    "\n",
    "# TODO: DEFINE LSTM LAYER (with hidden_size=100)\n",
    "x = LSTM(units=hidden_size,\n",
    "         dropout=0.,\n",
    "         recurrent_dropout=0.,\n",
    "         kernel_initializer='glorot_uniform',\n",
    "         recurrent_initializer='orthogonal',\n",
    "         return_sequences=False,\n",
    "         name='lstm')(input_sequences)\n",
    "\n",
    "# TODO: DEFINE DENSE LAYER (with hidden_size=100)\n",
    "x = Dense(units=100, activation='relu', name='fc')(x)\n",
    "\n",
    "# TODO: DEFINE SOFTMAX LAYER (10 classes)\n",
    "prediction = Dense(units=10, activation='softmax', name='prediction')(x)\n",
    "\n",
    "# INSTANTIATE MODEL\n",
    "model = Model(inputs=input_sequences,\n",
    "              outputs=prediction,\n",
    "              name='LSTM_mnist')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 4: Define callbacks\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoints\n",
    "ckpt_path = path+'/lstm_mnist_ckpts/lstm_mnist.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "if not os.path.exists(os.path.dirname(ckpt_path)):\n",
    "    os.makedirs(os.path.dirname(ckpt_path))\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=ckpt_path,\n",
    "                             monitor='val_acc',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "# Stop training early\n",
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "callbacks.append(earlystopping)\n",
    "\n",
    "# Reduce learning rate when learning does not improve\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=0.1, \n",
    "                             patience=10,\n",
    "                             verbose=1)\n",
    "callbacks.append(reducelr)\n",
    "\n",
    "# Tensorboard for visualization; only available with tensorflow backend\n",
    "# In the terminal; tensorboard --logdir='/full/path/to/lstm_mnist_logs/'\n",
    "if K.backend() == 'tensorflow':\n",
    "    logging.info('Using tensorboard callback')\n",
    "    tb_logdir = path+'/lstm_mnist_logs/'\n",
    "    if not os.path.exists(tb_logdir):\n",
    "        os.makedirs(tb_logdir)\n",
    "    tensorboard = TensorBoard(log_dir=tb_logdir,\n",
    "                              histogram_freq=1,\n",
    "                              write_graph=True)\n",
    "    callbacks.append(tensorboard)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 5: Compile & train model\n",
    "\n",
    "# TODO: COMPILE MODEL\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 6: Save & load model weights\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(path+'/weights/lstm_mnist_weights.h5')\n",
    "\n",
    "# Load model weights\n",
    "model.load_weights(path+'/weights/lstm_mnist_weights.h5')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 7: Test model performance\n",
    "\n",
    "test_scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "logging.info('Test accuracy: %.2f%%' %(test_scores[1] * 100))\n",
    "#print(\"Test accuracy: %.2f%%\" % (test_scores[1] * 100))\n",
    "\n",
    "#train_scores = model.evaluate(X_train, y_train, verbose=1)\n",
    "#print(\"Train accuracy: %.2f%%\" % (train_scores[1] * 100))\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 8: Using best checkpoint model\n",
    "\n",
    "best_model_path = path+'/lstm_mnist_ckpts/lstm_mnist.08-1.00.hdf5' # must change filename\n",
    "best_model = load_model(best_model_path)\n",
    "best_model.summary()\n",
    "test_scores = best_model.evaluate(X_test, Y_test, verbose=1)\n",
    "logging.info('Test accuracy: %.2f%%' %(test_scores[1] * 100))\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[INFO:77] 2017-07-06 01:14:35,755 > MNIST data has been loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:98] 2017-07-06 01:14:35,867 > final training data shape: (60000, 28, 28)\n",
      "[INFO:99] 2017-07-06 01:14:35,868 > final test data shape: (10000, 28, 28)\n",
      "[INFO:106] 2017-07-06 01:14:35,870 > final train label shape: (60000,)\n",
      "[INFO:107] 2017-07-06 01:14:35,870 > final test label shape: (10000,)\n",
      "[INFO:195] 2017-07-06 01:14:36,098 > Using tensorboard callback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sequence (InputLayer)  (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 28, 100)           51600     \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 143,110\n",
      "Trainable params: 143,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "INFO:tensorflow:Summary name lstm1/kernel:0 is illegal; using lstm1/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,453 > Summary name lstm1/kernel:0 is illegal; using lstm1/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm1/recurrent_kernel:0 is illegal; using lstm1/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,455 > Summary name lstm1/recurrent_kernel:0 is illegal; using lstm1/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm1/bias:0 is illegal; using lstm1/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,457 > Summary name lstm1/bias:0 is illegal; using lstm1/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm2/kernel:0 is illegal; using lstm2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,459 > Summary name lstm2/kernel:0 is illegal; using lstm2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm2/recurrent_kernel:0 is illegal; using lstm2/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,460 > Summary name lstm2/recurrent_kernel:0 is illegal; using lstm2/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm2/bias:0 is illegal; using lstm2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,462 > Summary name lstm2/bias:0 is illegal; using lstm2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,464 > Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,465 > Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,467 > Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-06 01:14:37,468 > Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9619Epoch 00000: val_acc improved from -inf to 0.98723, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.00-0.99.hdf5\n",
      "54000/54000 [==============================] - 19s - loss: 0.1035 - acc: 0.9619 - val_loss: 0.0361 - val_acc: 0.9872\n",
      "Epoch 2/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9896Epoch 00001: val_acc improved from 0.98723 to 0.99213, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.01-0.99.hdf5\n",
      "54000/54000 [==============================] - 19s - loss: 0.0301 - acc: 0.9896 - val_loss: 0.0226 - val_acc: 0.9921\n",
      "Epoch 3/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9937Epoch 00002: val_acc improved from 0.99213 to 0.99565, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.02-1.00.hdf5\n",
      "54000/54000 [==============================] - 19s - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0145 - val_acc: 0.9957\n",
      "Epoch 4/10\n",
      "53760/54000 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9954Epoch 00003: val_acc improved from 0.99565 to 0.99678, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.03-1.00.hdf5\n",
      "54000/54000 [==============================] - 21s - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0098 - val_acc: 0.9968\n",
      "Epoch 5/10\n",
      "53760/54000 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9964Epoch 00004: val_acc did not improve\n",
      "54000/54000 [==============================] - 19s - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0117 - val_acc: 0.9962\n",
      "Epoch 6/10\n",
      "53760/54000 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9970Epoch 00005: val_acc did not improve\n",
      "54000/54000 [==============================] - 19s - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "Epoch 7/10\n",
      "53760/54000 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9974Epoch 00006: val_acc improved from 0.99678 to 0.99718, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.06-1.00.hdf5\n",
      "54000/54000 [==============================] - 19s - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0098 - val_acc: 0.9972\n",
      "Epoch 8/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9978Epoch 00007: val_acc improved from 0.99718 to 0.99760, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_mnist_ckpts/lstm_mnist.07-1.00.hdf5\n",
      "54000/54000 [==============================] - 19s - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0073 - val_acc: 0.9976\n",
      "Epoch 9/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9981Epoch 00008: val_acc did not improve\n",
      "54000/54000 [==============================] - 19s - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0093 - val_acc: 0.9971\n",
      "Epoch 10/10\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9983Epoch 00009: val_acc did not improve\n",
      "54000/54000 [==============================] - 19s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0099 - val_acc: 0.9968\n",
      "10000/10000 [==============================] - 3s     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:244] 2017-07-06 01:17:59,147 > Test accuracy: 99.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sequence (InputLayer)  (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 28, 100)           51600     \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 143,110\n",
      "Trainable params: 143,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9856/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:260] 2017-07-06 01:18:03,714 > Test accuracy: 99.71%\n"
     ]
    }
   ],
   "source": [
    "## 2 LSTM Layer\n",
    "## RNN 은 layer를 많이 쌓는다고 성능이 크게 향상되지 않는 경향\n",
    "\n",
    "### Step 1: Import modules\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "## Fix random seed for reproducibility\n",
    "np.random.seed(20170704)\n",
    "\n",
    "\n",
    "## Check proper working directory\n",
    "#os.chdir('path/to/day_2/')\n",
    "#if os.getcwd().split('/')[-1] == 'day_2':\n",
    "#    pass\n",
    "#else:\n",
    "#    raise OSError('Check current working directory.\\n'\n",
    "#                  'If not specified as instructed, '\n",
    "#                  'more errors will occur throught the code.\\n'\n",
    "#                  '- Current working directory: %s' % os.getcwd())\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "\n",
    "## Set logging\n",
    "def set_logging(testlog=False):\n",
    "    # 1. Make 'logger' instance\n",
    "    logger = logging.getLogger()\n",
    "    # 2. Make 'formatter'\n",
    "    formatter = logging.Formatter(\n",
    "            '[%(levelname)s:%(lineno)s] %(asctime)s > %(message)s'\n",
    "            )\n",
    "    # 3. Make 'streamHandler'\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    # 4. Set 'formatter' to 'streamHandler'\n",
    "    streamHandler.setFormatter(formatter)\n",
    "    # 5. Add streamHandler to 'logger' instance\n",
    "    logger.addHandler(streamHandler)\n",
    "    # 6. Set level of log; DEBUG -> INFO -> WARNING -> ERROR -> CRITICAL\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # 7. Print test INFO message\n",
    "    if testlog: # default is 'False'\n",
    "        logging.info(\"Stream logging available!\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "_ = set_logging()\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 2: Load & preprocess data\n",
    "\n",
    "## 2-1. Load\n",
    "# Data, shuffled and split between train / test sets\n",
    "# shape of X_train, X_test; (batch_size, height, width)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "logging.info('MNIST data has been loaded.')\n",
    "\n",
    "## 2-2. Preprocess\n",
    "# Change data types to 'float32'\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "# Normalization\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Check input shape of data; (batch_size, timesteps, input_dim)\n",
    "timesteps = 28\n",
    "input_dim = 28\n",
    "#timesteps = 14\n",
    "#input_dim = 56\n",
    "\n",
    "X_train = X_train.reshape(-1, timesteps, input_dim) # -1: reshape 할때, 알아서 채우라는 의미\n",
    "X_test = X_test.reshape(-1, timesteps, input_dim)\n",
    "\n",
    "logging.info('final training data shape: {}'.format(X_train.shape))\n",
    "logging.info('final test data shape: {}'.format(X_test.shape))\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot vectors)\n",
    "num_classes = 10\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "logging.info('final train label shape: {}'.format(y_train.shape))\n",
    "logging.info('final test label shape: {}'.format(y_test.shape))\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 3: Build Model\n",
    "\n",
    "## 3-1. Define hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "\n",
    "## 3-2. Define RNN model with LSTM cells for MNIST data\n",
    "\n",
    "# TODO: DEFINE INPUT TENSOR (hint; (timesteps, input_dim))\n",
    "input_sequences = Input(shape=(timesteps, input_dim), name='input_sequence')\n",
    "\n",
    "# TODO: DEFINE LSTM LAYER (with hidden_size=100)\n",
    "x = LSTM(units=hidden_size,\n",
    "         dropout=0.,\n",
    "         recurrent_dropout=0.,\n",
    "         kernel_initializer='glorot_uniform',\n",
    "         recurrent_initializer='orthogonal',\n",
    "         return_sequences=True,\n",
    "         name='lstm1')(input_sequences)\n",
    "\n",
    "x = LSTM(units=hidden_size,\n",
    "         dropout=0.,\n",
    "         recurrent_dropout=0.,\n",
    "         kernel_initializer='glorot_uniform',\n",
    "         recurrent_initializer='orthogonal',\n",
    "         return_sequences=False,\n",
    "         name='lstm2')(x)\n",
    "\n",
    "# TODO: DEFINE DENSE LAYER (with hidden_size=100)\n",
    "x = Dense(units=100, activation='relu', name='fc')(x)\n",
    "\n",
    "# TODO: DEFINE SOFTMAX LAYER (10 classes)\n",
    "prediction = Dense(units=10, activation='softmax', name='prediction')(x)\n",
    "\n",
    "# INSTANTIATE MODEL\n",
    "model = Model(inputs=input_sequences,\n",
    "              outputs=prediction,\n",
    "              name='LSTM_mnist_2')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 4: Define callbacks\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoints\n",
    "ckpt_path = path+'/lstm_mnist_ckpts/lstm_mnist.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "if not os.path.exists(os.path.dirname(ckpt_path)):\n",
    "    os.makedirs(os.path.dirname(ckpt_path))\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=ckpt_path,\n",
    "                             monitor='val_acc',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "# Stop training early\n",
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "callbacks.append(earlystopping)\n",
    "\n",
    "# Reduce learning rate when learning does not improve\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=0.1, \n",
    "                             patience=10,\n",
    "                             verbose=1)\n",
    "callbacks.append(reducelr)\n",
    "\n",
    "# Tensorboard for visualization; only available with tensorflow backend\n",
    "# In the terminal; tensorboard --logdir='/full/path/to/lstm_mnist_logs/'\n",
    "if K.backend() == 'tensorflow':\n",
    "    logging.info('Using tensorboard callback')\n",
    "    tb_logdir = path+'/lstm_mnist_logs/'\n",
    "    if not os.path.exists(tb_logdir):\n",
    "        os.makedirs(tb_logdir)\n",
    "    tensorboard = TensorBoard(log_dir=tb_logdir,\n",
    "                              histogram_freq=1,\n",
    "                              write_graph=True)\n",
    "    callbacks.append(tensorboard)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 5: Compile & train model\n",
    "\n",
    "# TODO: COMPILE MODEL\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 6: Save & load model weights\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(path+'/weights/lstm_mnist_weights2.h5')\n",
    "\n",
    "# Load model weights\n",
    "model.load_weights(path+'/weights/lstm_mnist_weights2.h5')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 7: Test model performance\n",
    "\n",
    "test_scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "logging.info('Test accuracy: %.2f%%' %(test_scores[1] * 100))\n",
    "#print(\"Test accuracy: %.2f%%\" % (test_scores[1] * 100))\n",
    "\n",
    "#train_scores = model.evaluate(X_train, y_train, verbose=1)\n",
    "#print(\"Train accuracy: %.2f%%\" % (train_scores[1] * 100))\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 8: Using best checkpoint model\n",
    "\n",
    "best_model_path = path+'/lstm_mnist_ckpts/lstm_mnist.08-1.00.hdf5' # must change filename\n",
    "best_model = load_model(best_model_path)\n",
    "best_model.summary()\n",
    "test_scores = best_model.evaluate(X_test, Y_test, verbose=1)\n",
    "logging.info('Test accuracy: %.2f%%' %(test_scores[1] * 100))\n",
    "\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
