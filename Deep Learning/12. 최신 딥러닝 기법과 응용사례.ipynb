{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Model 의 성능\n",
    "\n",
    "ImageNet Classification top-5 error (%)\n",
    "\n",
    "\n",
    "## Deep Learning 의 연구 트렌드\n",
    "\n",
    "\n",
    "## 모델을 깊게 하기 위해 필요한 것들\n",
    "\n",
    "- Batch Normalization (2015)\n",
    "- Residual Network (2016)\n",
    "- Skip Connection\n",
    "\n",
    "\n",
    "## 1. Batch Normalization\n",
    "\n",
    "- ** 최근에는 딥러닝 모델의 정석** 처럼 사용됨\n",
    "- Normalization\n",
    "\n",
    "- 모델의 학습이 어려운 이유는 batch 사이의 분포 (covariance)가 서로 다르기 떄문이라는 가정\n",
    "- Batch 단위로 normalization을 수행하여 이를 보정\n",
    "- 학습속도 향상 (high learning rate)\n",
    "- 파라미터 초기값에 강건함\n",
    "- 예측 성능의 향상 (Improve generalization)\n",
    "\n",
    "\n",
    "- Normalization 한 batch Data를 선형식 변환 $(\\gamma, \\beta)$\n",
    "- 각각의 변수마다 적용\n",
    "\n",
    "- 테스트를 수행할 때는 학습한 $\\gamma, \\beta$ 와 전체 학습 데이터의 평균과 표준편차를 이용해 예측함\n",
    "\n",
    "- Batch Normalization은 언제 어디에다가 넣어야 할까?\n",
    "  - activation function 이전에 적용\n",
    "  - activation function 옵션을 입력을 하지 않으면 그냥 linear 하게 output을 내보내게 됨\n",
    "  - 이 다음에 batch normalization을 하고, 그 다음에 activation function(e.g. relu) 을 적용\n",
    "  \n",
    "- Learning rate 을 30배 향상 $\\rightarrow$ 학습 속도 향상\n",
    "\n",
    "\n",
    "\n",
    "## 2. Residual Network & Skip Connection\n",
    "\n",
    "* 레이어의 수를 많이 쌓으면 성능이 저하됨 -> 이 문제를 해결한 기법 : Residual Network\n",
    "  - 높은 train error : 학습이 잘 되지 않음 (optimization 문제)\n",
    "  - 높은 test error : 일반화 성능이 떨어짐 (overfitting 문제)\n",
    "\n",
    "- Identity mapping 과 residual learning 그리고 skip connection (shortcut)\n",
    "- Residual: $F(x) = H(x) -x$\n",
    "- $F(x): x$ 를 제외하고 부족한 부분을 모델링\n",
    "\n",
    "> 2015, https://arxiv.org/pdf/1512.03385.pdf <br/>\n",
    "> 2016, https://arxiv.org/pdf/1603.05027.pdf <br/>\n",
    "> Slide, http://kaiminghe.com/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\n",
    "\n",
    "ref : http://funmv2013.blogspot.kr/2016/09/resnet.html\n",
    "\n",
    "\n",
    "### Residual Network 구조\n",
    "\n",
    "- Identity mapping 과 residual learning 그리고 skip connectinon (shortcut)\n",
    "- Residual : $F(x) = H(x) - x$\n",
    "- $F(x):x $ 를 제외하고 부족한 부분을 모델링 \n",
    "\n",
    "- VGGNet 스타일 네트워크 구조\n",
    "  - all $3*3$ conv. layers\n",
    "  - saze/2 $\\right arrow$ # filter $*$2\n",
    "  - batch normalization after every conv. layer\n",
    "  - no hidden fully-connected layers\n",
    "  - no dropout\n",
    "  - 레이어 아주 많이 쌓기\n",
    "\n",
    "- 계산 비교\n",
    "  - all $(3*3)$\n",
    "  - Resnet\n",
    "  \n",
    "  \n",
    "  \n",
    "skip connection 구조가 vanishing gradient 문제를 해결하는 이유 : 블락단위로 weight를 거치는 데 \"+\" 합으로 연결\n",
    "으로 연결되면\n",
    "+으로 연결되면 \n",
    "상위 레이어의 gradient 를 하위 레이어에 적은 손실로 전달\n",
    "\n",
    "\n",
    "## - Google의 Inception 모델\n",
    "\n",
    "- 구글에서 발표한 모델로 계속 업데이트 되고 있음 (version 1~4)\n",
    "  - 메인 아이디어는 convolutional layer에다가 서로 다른 정보를 넣어줌\n",
    "  - 파라미터가 많아짐\n",
    "  - Mixture of Experts 앙상블 기법\n",
    "- Deep & wide network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* 같은 task를 수행할 수 있으면 기왕이면 단순한게 좋다 : simple is best (오캄의 면도날)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
