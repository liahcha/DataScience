{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for binary sentiment classification, on IMDB\n",
    "\n",
    "- 50,000 movie reviews\n",
    "- 25,000 for training, 25,000 for testing\n",
    "- Positive reviews labled with 1\n",
    "- Negative reviews labled with 0\n",
    "- Obtainable at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "**일반적인 RNN보다 LSTM이 파라미터가 4배 더 많음 (Gate 갯수가 더 있음)** <br/>\n",
    "**GRU는 파라미터 갯수가 LSTM의 반 ** (속도면에서 LSTM보다 빠르고, 성능면에서 많이 차이 나지 않기 때문에 GRU도 많이 사용함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[INFO:66] 2017-07-05 23:58:39,705 > Loading imdb dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:72] 2017-07-05 23:58:42,464 > Shape of train data: (25000,)\n",
      "[DEBUG:73] 2017-07-05 23:58:42,465 > Shape of test data: (25000,)\n",
      "[INFO:80] 2017-07-05 23:58:42,504 > Vocabulary size: 88584\n",
      "[INFO:89] 2017-07-05 23:58:43,257 > \n",
      "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s and with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with and film want an\n",
      "- pos\n",
      "[INFO:90] 2017-07-05 23:58:43,258 > \n",
      "the much was me of lack in change as you of because hard was nothing does when in stuff movie was best least of student to should not when was off here's else who doesn't of how didn't since it for as you it of sure was one your me of and in can i i was saying was after own that as you is breaking major of how didn't since than do not movie sometimes movie that and of slightly scenes was children in out aka is and that with had zombie bit of before nothing one think becomes more with even now their was you're good japanese for trouble make very what have one is over flick in notorious but impressed since won't not had such real bad and history in as roles br you make can't death quality and to abandoned must they where to quest are death excellent instead in gordon 2001 br make my was rather woman is again great this until into at some are of reviewers watch as you and i i other don't well and great own as on is didn't was let also now especially for lines as on is didn't given there good absurd dick to it's art but of you prison as on real forget they later already didn't john br make corny am either didn't basically in of great for and that didn't video serious\n",
      "- neg\n",
      "[INFO:107] 2017-07-05 23:58:44,037 > Pad sequences shorter than 500 with \"0\"\n",
      "[INFO:108] 2017-07-05 23:58:44,037 > Truncate sequences longer than 500 to 500\n",
      "[DEBUG:109] 2017-07-05 23:58:44,038 > Shape of train data (preprocessed): (25000, 500)\n",
      "[DEBUG:110] 2017-07-05 23:58:44,038 > Shape of test data (preprocessed) : (25000, 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sequence (InputLayer)  (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 223,401\n",
      "Trainable params: 223,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0 is illegal; using embedding/embeddings_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,130 > Summary name embedding/embeddings:0 is illegal; using embedding/embeddings_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm/kernel:0 is illegal; using lstm/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,132 > Summary name lstm/kernel:0 is illegal; using lstm/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm/recurrent_kernel:0 is illegal; using lstm/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,134 > Summary name lstm/recurrent_kernel:0 is illegal; using lstm/recurrent_kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm/bias:0 is illegal; using lstm/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,136 > Summary name lstm/bias:0 is illegal; using lstm/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,139 > Summary name fc/kernel:0 is illegal; using fc/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,141 > Summary name fc/bias:0 is illegal; using fc/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,144 > Summary name prediction/kernel:0 is illegal; using prediction/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:82] 2017-07-05 23:58:45,146 > Summary name prediction/bias:0 is illegal; using prediction/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "22400/22500 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7339Epoch 00000: val_acc improved from -inf to 0.81360, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_imdb_ckpts/weights.00-0.81.hdf5\n",
      "22500/22500 [==============================] - 212s - loss: 0.5288 - acc: 0.7346 - val_loss: 0.6366 - val_acc: 0.8136\n",
      "Epoch 2/5\n",
      "22400/22500 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8652Epoch 00001: val_acc improved from 0.81360 to 0.86760, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_imdb_ckpts/weights.01-0.87.hdf5\n",
      "22500/22500 [==============================] - 198s - loss: 0.3308 - acc: 0.8653 - val_loss: 0.3257 - val_acc: 0.8676\n",
      "Epoch 3/5\n",
      "22400/22500 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.8871Epoch 00002: val_acc did not improve\n",
      "22500/22500 [==============================] - 197s - loss: 0.2835 - acc: 0.8872 - val_loss: 0.3787 - val_acc: 0.8284\n",
      "Epoch 4/5\n",
      "22400/22500 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.8962Epoch 00003: val_acc improved from 0.86760 to 0.87640, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_imdb_ckpts/weights.03-0.88.hdf5\n",
      "22500/22500 [==============================] - 209s - loss: 0.2620 - acc: 0.8963 - val_loss: 0.3034 - val_acc: 0.8764\n",
      "Epoch 5/5\n",
      "22400/22500 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9072Epoch 00004: val_acc improved from 0.87640 to 0.88120, saving model to /home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata/lstm_imdb_ckpts/weights.04-0.88.hdf5\n",
      "22500/22500 [==============================] - 211s - loss: 0.2420 - acc: 0.9073 - val_loss: 0.3166 - val_acc: 0.8812\n",
      "25000/25000 [==============================] - 284s   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:251] 2017-07-06 00:20:39,340 > Test accuracy: 86.39%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 1: Import modules & set logging\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "## Fix random seed for reproducibility\n",
    "np.random.seed(20170704)\n",
    "\n",
    "\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "## Set logging\n",
    "def set_logging(testlog=False):\n",
    "    # 1. Make 'logger' instance\n",
    "    logger = logging.getLogger()\n",
    "    # 2. Make 'formatter'\n",
    "    formatter = logging.Formatter(\n",
    "            '[%(levelname)s:%(lineno)s] %(asctime)s > %(message)s'\n",
    "            )\n",
    "    # 3. Make 'streamHandler'\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    # 4. Set 'formatter' to 'streamHandler'\n",
    "    streamHandler.setFormatter(formatter)\n",
    "    # 5. Add streamHandler to 'logger' instance\n",
    "    logger.addHandler(streamHandler)\n",
    "    # 6. Set level of log; DEBUG -> INFO -> WARNING -> ERROR -> CRITICAL\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # 7. Print test INFO message\n",
    "    if testlog: # default is 'False'\n",
    "        logging.info(\"Stream logging available!\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "_ = set_logging()\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 2: Load, view & preprocess data\n",
    "\n",
    "## 2-1. Load\n",
    "# Load dataset, but only keep the top n words\n",
    "logging.info('Loading imdb dataset...')\n",
    "top_words = 5000 \n",
    "# 빈도수 높은 단어 5,000개만 끊어서 (index값) 사용 \n",
    "# 5,000 dimensional one-hot vector\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "logging.debug('Shape of train data: {}'.format(X_train.shape))\n",
    "logging.debug('Shape of test data: {}'.format(X_test.shape))\n",
    "\n",
    "\n",
    "## 2-2. View\n",
    "# word2idx, idx2word <- python dictionaries\n",
    "word2idx = imdb.get_word_index()\n",
    "idx2word = dict((v, k) for k, v in word2idx.items())\n",
    "# index가 1부터 시작하는 index, word(key, value) 형태의 dictionary (index 0 은 아래에 padding 해줌)\n",
    "logging.info('Vocabulary size: {}'.format(len(idx2word)))\n",
    "\n",
    "# View the original review in text\n",
    "# index word를 넣어서 text로 역변환해주는 함수 (빈번하게 등장하는 5,000개만)\n",
    "def to_text(X, idx2word):\n",
    "    text = [' '.join([idx2word[index] for index in review])for review in X]\n",
    "    return text\n",
    "\n",
    "text_train = to_text(X_train, idx2word)\n",
    "text_test = to_text(X_test, idx2word)\n",
    "logging.info('\\n{}\\n- {}'.format(text_train[0], ('pos' if y_train[0] == 1 else 'neg')))\n",
    "logging.info('\\n{}\\n- {}'.format(text_test[245], ('pos' if y_test[245] == 1 else 'neg')))\n",
    "\n",
    "\n",
    "## 2-3. Preprocess\n",
    "# Truncate and pad input sequences\n",
    "max_review_len = 500 \n",
    "# input length의 길이는 500으로 고정 (한 문장의 단위를 500 단어까지로 정의)\n",
    "# 500보다 긴것은 500에서 잘라주고, 500보다 짧은 것은 나머지를 0으로 채워 줌\n",
    "\n",
    "if X_train.shape == (25000, ) and X_test.shape == (25000, ):\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_review_len,\n",
    "                                     padding='pre', truncating='pre',\n",
    "                                     value=0.)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_review_len,\n",
    "                                    padding='pre', truncating='pre',\n",
    "                                    value=0.)\n",
    "# padding='pre' 0을 앞에 붙여줌\n",
    "# : 만약 뒤에 0을 붙이게 되면, vanishing gradient 문제 + 앞에서 계산된 것이 뒤에서 손실될 수 있음\n",
    "# truncating='pre' 문장을 앞에서 잘라줌 : 보통 결론을 뒤에 적는 경우가 많기 때문\n",
    "logging.info('Pad sequences shorter than %d with \"0\"' % max_review_len)\n",
    "logging.info('Truncate sequences longer than {0} to {0}'.format(max_review_len))\n",
    "logging.debug('Shape of train data (preprocessed): {}'.format(X_train.shape))\n",
    "logging.debug('Shape of test data (preprocessed) : {}'.format(X_test.shape))\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 3: Build model\n",
    "\n",
    "## 3-1. Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "hidden_size = 100\n",
    "embedding_vector_len = 32 \n",
    "# 5,000 짜리 one-hot vector를 넣어주면, embedding layer를 통과하면 32 dimension 으로 변형됨\n",
    "# dimension이 너무 크므로 32 dimension으로 embedding을 시킴\n",
    "\n",
    "## 3-2. Define RNN model with LSTM cells for IMDB data\n",
    "\n",
    "# Define input (SHAPE IS IMPORTANT!!!)\n",
    "input_sequence = Input(shape=(max_review_len, ), # max_review_len = 500 # LST의 timestep unfold 수\n",
    "                       dtype='int32', \n",
    "                       name='input_sequence')\n",
    "\n",
    "\n",
    "# Define Embedding layer (단어의 index를 넣어줌)\n",
    "x = Embedding(input_dim=top_words, # top_words = 5000  : one-hot vector의 크기\n",
    "              output_dim=embedding_vector_len, # embedding_vetor_len = 32\n",
    "              input_length=max_review_len, # max_review_len = 500 : LSTM timestep 길이와 동일\n",
    "              mask_zero=True, # MASK를 씌워놓으면 Gradient 계산을 하지 않음 (zero <- padding 0 으로 들어간 가짜값)\n",
    "              name='embedding')(input_sequence)\n",
    "\n",
    "\n",
    "# Define LSTM layer\n",
    "x = LSTM(units=hidden_size,\n",
    "         dropout=0.,\n",
    "         recurrent_dropout=0., # overfitting 방지하기 위해 dropout을 사용하는데, 이 옵션은 사용하지 않는 것을 권고함\n",
    "                               # LSTM이 vanishing gradient 문제를 해결하기 위한 모델이므로, 다음 timestep에 최대한 많은 정보를 포함하기 위해\n",
    "         kernel_initializer='glorot_uniform', # 초기값을 잘 잡아줌\n",
    "         recurrent_initializer='orthogonal',  # identity matrix를 사용\n",
    "         return_sequences=False, \n",
    "         # 현재 모델은 LSTM 을 한 layer만 쌓았는데 여러 layer를 쌓을 수 있으므로, \n",
    "         # 두번째 LSTM에서 이전 layer의 output을 input으로 받을 것인지 아닌지에 대한 옵션  (TRUE 면 받을 수 있음, FALSE면 마지막 단계 Y 만 출력)\n",
    "         name='lstm')(x)\n",
    "\n",
    "\n",
    "# Define Dense layer\n",
    "x = Dense(units=100, activation='relu', name='fc')(x)\n",
    "\n",
    "\n",
    "# Define prediction layer; use sigmoid for binary classification\n",
    "prediction = Dense(units=1, activation='sigmoid', name='prediction')(x)\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = Model(inputs=input_sequence,\n",
    "              outputs=prediction,\n",
    "              name='LSTM_imdb')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 4: Define callbacks\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoints\n",
    "ckpt_path = path+'/lstm_imdb_ckpts/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "if not os.path.exists(os.path.dirname(ckpt_path)):\n",
    "    os.makedirs(os.path.dirname(ckpt_path))\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=ckpt_path,\n",
    "                             monitor='val_acc',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "# Stop training early\n",
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "callbacks.append(earlystopping)\n",
    "\n",
    "# Reduce learning rate when learning does not improve\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=0.1, \n",
    "                             patience=10,\n",
    "                             verbose=1)\n",
    "callbacks.append(reducelr)\n",
    "\n",
    "# Tensorboard for visualization\n",
    "if K.backend() == 'tensorflow':\n",
    "    tb_logdir = path+'/lstm_imdb_logs/'\n",
    "    if not os.path.exists(tb_logdir):\n",
    "        os.makedirs(tb_logdir)\n",
    "    tensorboard = TensorBoard(log_dir=tb_logdir,\n",
    "                              histogram_freq=1,\n",
    "                              write_graph=True)\n",
    "    callbacks.append(tensorboard)\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 5: Compile & train model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', #  ADAM이 일반적으로 좋지만, RNN에서는 이 optimizer도 잘되는 것으로 알려져 있음\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 6: Save & load weights\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(path+'/weights/lstm_imdb_weights.h5')\n",
    "\n",
    "# Load model weights\n",
    "model.load_weights(path+'/weights/lstm_imdb_weights_master.h5')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "### Step 7: Test final model performance\n",
    "\n",
    "test_scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "logging.info('Test accuracy: %.2f%%' %(test_scores[1] * 100))\n",
    "#print(\"Test accuracy: %.2f%%\" % (test_scores[1] * 100))\n",
    "\n",
    "#train_scores = model.evaluate(X_train, y_train, verbose=1)\n",
    "#print(\"Train accuracy: %.2f%%\" % (train_scores[1] * 100))\n",
    "\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
