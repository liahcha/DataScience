{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Model 의 성능\n",
    "\n",
    "- ImageNet Classification top-5 error (%)\n",
    "\n",
    "![](./img/12_deep_model.JPG)\n",
    "\n",
    "## Deep Learning 의 연구 트렌드\n",
    "- 점점더 deep\n",
    "![](./img/12_dl_trend.JPG)\n",
    "\n",
    "\n",
    "### 모델을 깊게 하기 위해 필요한 것들\n",
    "\n",
    "- Batch Normalization (2015)\n",
    "- Residual Network (2016)\n",
    "- Skip Connection\n",
    "\n",
    "\n",
    "## 1. Batch Normalization\n",
    "\n",
    "- ** 최근에는 딥러닝 모델의 정석** 처럼 사용되는 추세\n",
    "\n",
    "#### Normalization\n",
    "$\\hat{x} = \\frac{x - \\mu}{\\sigma}$ <br/><br/>\n",
    "$\\mu = \\frac{1}{N}\\sigma-{i} x_{i}$  : 평균 <br/><br/>\n",
    "$\\omega = \\sqrt{\\frac{\\sum_{i}(x-\\mu)^{2}}{N}} : 표준편차$\n",
    "\n",
    "#### Batch\n",
    "![](./img/12_batch.JPG)\n",
    "\n",
    "<br/>\n",
    "> Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167. <br/>\n",
    "> https://arxiv.org/abs/1502.03167\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "- 모델의 학습이 어려운 이유는 batch 사이의 분포 (covariance)가 서로 다르기 때문이라는 가정\n",
    "- Batch 단위로 normalization을 수행하여 이를 보정\n",
    "- 학습속도 향상 (high learning rate)\n",
    "- 파라미터 초기값에 강건함\n",
    "- 예측 성능의 향상 (Improve generalization)\n",
    "\n",
    "![](./img/12_mini_batch.JPG)\n",
    "\n",
    "- Normalization 한 batch Data를 선형식 변환 $(\\gamma, \\beta)$\n",
    "- 각각의 변수마다 적용\n",
    "![](./img/12_batch_normalization.JPG)\n",
    "\n",
    "- 테스트를 수행할 때는 학습한 $\\gamma, \\beta$ 와 전체 학습 데이터의 평균과 표준편차를 이용해 예측함\n",
    "\n",
    "![](./img/12_batch_normalization_02.JPG)\n",
    "\n",
    "#### Batch Normalization은 언제 어디에다가 넣어야 할까?\n",
    "\n",
    "- activation function 이전에 적용\n",
    "  - activation function 옵션을 입력을 하지 않으면 그냥 linear 하게 output을 내보내게 됨\n",
    "  - batch normalization을 하고, 그 다음에 activation function(e.g. relu) 을 적용\n",
    "- convolution layer에서는 각각의 채널, 필터에 대해 적용\n",
    "\n",
    "![](./img/12_batch_normalization_03.JPG)\n",
    "\n",
    "#### Batch Normaaliztion을 적용하면\n",
    "\n",
    "- Learning rate 을 30배 향상 $\\rightarrow$ 학습 속도 향상\n",
    "\n",
    "![](./img/12_batch_normalization_04.JPG)\n",
    "\n",
    "\n",
    "## 2. Residual Network & Skip Connection\n",
    "\n",
    "* 아무리 '딥러닝' 이라도, 레이어의 수를 많이 쌓으면 성능이 저하됨 $\\rightarrow$ 이 문제를 해결한 기법 : Residual Network\n",
    "  - 높은 train error : 학습이 잘 되지 않음 (optimization 문제)\n",
    "  - 높은 test error : 일반화 성능이 떨어짐 (overfitting 문제) <br/><br/>\n",
    "![](./img/12_rn_01.JPG)\n",
    "\n",
    "## Residual Network\n",
    "> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision\n",
    "and Pattern Recognition (pp. 770-778). <br/>\n",
    "> - 2015, https://arxiv.org/pdf/1512.03385.pdf <br/>\n",
    "> He, K., Zhang, X., Ren, S., & Sun, J. (2016, October). Identity mappings in deep residual networks. In European Conference on Computer Vision (pp. 630-\n",
    "645). Springer International Publishing. <br/>\n",
    "> - 2016, https://arxiv.org/pdf/1603.05027.pdf <br/>\n",
    "> - Slide, http://kaiminghe.com/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf <br/>\n",
    "> - ref : http://funmv2013.blogspot.kr/2016/09/resnet.html\n",
    "\n",
    "- Identity mapping 과 residual learning 그리고 skip connection (shortcut)\n",
    "- Residual: $F(x) = H(x) -x$\n",
    "- $F(x): x$ 를 제외하고 부족한 부분을 모델링\n",
    "![](./img/12_rn_02.JPG)\n",
    "\n",
    "\n",
    "### Residual Network 구조\n",
    "\n",
    "- Identity mapping 과 residual learning 그리고 skip connectinon (shortcut)\n",
    "- Residual : $F(x) = H(x) - x$\n",
    "- $F(x):x $ 를 제외하고 부족한 부분을 모델링 \n",
    "\n",
    "![](./img/12_rn_03.JPG) <br/>\n",
    "\n",
    "- VGGNet 스타일 네트워크 구조\n",
    "  - all $3 \\times 3$ conv. layers\n",
    "  - size/2 $ \\rightarrow $ # filter $\\times 2$\n",
    "  - batch normalization after every conv. layer\n",
    "  - no hidden fully-connected layers\n",
    "  - no dropout\n",
    "  - 레이어 아주 많이 쌓기\n",
    "  \n",
    "   ![](./img/12_rn_04.JPG)\n",
    "\n",
    "- 계산 비교\n",
    "  - all $(3*3)$\n",
    "  - Resnet\n",
    "  ![](./img/12_rn_05.JPG)\n",
    "  ![](./img/12_rn_06.JPG)\n",
    "  \n",
    "#### Residual Network 성능\n",
    "\n",
    "![](./img/12_rn_07.JPG)\n",
    "\n",
    "\n",
    "## Skip Connection\n",
    "\n",
    "![](./img/12_rn_08.JPG)\n",
    "\n",
    "![](./img/12_shortcut_connection_02.JPG)\n",
    "![](./img/12_rn_09.JPG)\n",
    "\n",
    "- Vanishing gradient 현상을 피할 수 있음\n",
    "- skip connection 구조가 vanishing gradient 문제를 해결하는 이유 \n",
    "  - 블락단위로 weight를 거치는 데 \"+\" 합으로 연결되면, 상위 레이어의 gradient 를 하위 레이어에 적은 손실로 전달\n",
    "\n",
    "$$x_{L} = x_{l} + \\sum^{L-1}_{i=l} F(x_{i}), \\hspace{1cm} l \\leq L $$  \n",
    "$$\\frac{\\partial E}{\\partial x_{l}} = \\frac{\\partial E}{\\partial x_{L}}\\frac{\\partial x_{L}}{\\partial x_{l}} = \\frac{\\partial E}{\\partial x_{L}}(1+\\frac{\\partial}{\\partial x_{l}} \\sum^{L-1}_{i=l} F(x_{i})$$\n",
    "\n",
    "- No shortcut의 경우,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial x_{l}} = \\prod ^{L-1}_{i=l} W_{i} \\frac{\\partial E}{\\partial x_{L}}$$\n",
    "  \n",
    "#### Shortcut Connection\n",
    "\n",
    "- 지름길을 만들기 위해 고려해야 할것 $dim(x_{l}) = dim(x_{l+1})$\n",
    "\n",
    "![](./img/12_shortcut_connection.JPG)\n",
    "\n",
    "- zero padding shortcut\n",
    "![](./img/12_shortcut_connection_01.JPG)\n",
    "\n",
    "#### Residual Network 성능\n",
    "\n",
    "\n",
    "- 현재 가장 좋은 성능을 보이는 모델\n",
    "\n",
    "![](./img/12_rn_10.JPG)\n",
    "\n",
    "\n",
    "## 3. Google의 Inception 모델\n",
    "\n",
    "> Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. (2016). Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261. <br/>\n",
    "> https://norman3.github.io/papers/docs/google_inception.html\n",
    "\n",
    "- 구글에서 발표한 모델로 계속 업데이트 되고 있음 (version 1~4)\n",
    "  - 메인 아이디어는 convolutional layer에다가 서로 다른 정보를 넣어줌\n",
    "  - 파라미터가 많아짐\n",
    "  - Mixture of Experts 앙상블 기법\n",
    "\n",
    "\n",
    "\n",
    "#### 같은 task를 수행할 수 있으면 기왕이면 단순한게 좋다 : simple is best (Occam's Razor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
