{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reinforcement Learning (강화학습)\n",
    "\n",
    "- Agent : 모델\n",
    "- Environment : 데이터 (state, reward)\n",
    "\n",
    "![](./img/23_rf.png)\n",
    "\n",
    "- 지도학습(supervised learning) 모델과의 차이점\n",
    "  - Non-iid samples \n",
    "  - 연속적인 의사결정에 활용\n",
    "  - 보상이 즉각적이지 않을 수 있음\n",
    "  \n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "\n",
    "- Reinforcement Learning 에 Depp Learing을 적용한 것\n",
    "- **Deep Q-Learning**\n",
    "  > Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533 <br/> <br/>\n",
    "  > Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 <br/>\n",
    "  \n",
    "- **Policy Gradient**\n",
    "  > Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15) (pp. 1889-1897) <br/> <br/>\n",
    "  > Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., ... & Kavukcuoglu, K. (2016, June). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (pp.1928-1937) <br/>\n",
    "  \n",
    "- 주로 게임환경을 이용하여 연구개발\n",
    "- Input : Raw-pixel frame (State가 매우 많음)\n",
    "- Output : Action (게임조작)\n",
    "\n",
    "\n",
    "## Markov Decision Process (MDP)\n",
    "\n",
    "- $\\gamma$ : discount factor\n",
    "  - 너무 많은 단계를 걸쳐서 가계되면 1보다 작은 값을 줌 (빠르게 clear 하기 위해서)\n",
    "  \n",
    "- Definition\n",
    "> A _Markov DEcision Precess_ is a tuple $<S, A >$ <br/>\n",
    ">   - S : finite set of states <br/>\n",
    "\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "- Definition\n",
    "  > A state $S_{t}$ is *Markov* if and only if <br/> <br/> \n",
    "  > $ \\mathbb{P}[S_{t+1}|S_{t}] = \\mathbb{P}[S_{t+1} | S_{1},  \\cdots , S_{t}]$ <br/>\n",
    "  - 다음 상태로 갈 떄는, 현재 정보가 가장 좋은 정보\n",
    "  \n",
    "### Markov Process\n",
    "\n",
    "\n",
    "### Markov Reward Process\n",
    "\n",
    "\n",
    "### Return\n",
    "\n",
    "- reward 들의 총합\n",
    "- return 을 maximize 하는 것이 목표\n",
    "\n",
    "\n",
    "\n",
    "## Policy\n",
    "\n",
    "- state 가 주어졌을때, Action 을 취할 확률\n",
    "- Return 을 최대화 하는 policy 가 가장 좋은 policy \n",
    "\n",
    "> A policy $\\pi$ is a distribution over actions given states, <br/><br/> \n",
    "> $\\pi (\\partial | s) = \\mathbb{P}[A_{t} = \\partial | S_{t} = s] $\n",
    "\n",
    "\n",
    "## Action-Value Function\n",
    "\n",
    "- $G_{t}$ return 의 총합\n",
    "- $q_{pi} (s, a) $ : 현재 상태  t 에서, action $\\partial$ 을 취했을때 Return 의 기대값\n",
    "\n",
    "\n",
    "## Bellmann Equation\n",
    "\n",
    "- Value Function\n",
    "\n",
    "### Bellmann Expectation Equations\n",
    "\n",
    "- $P^{\\partial}_{ss^{`}}$ : state transition probability \n",
    "\n",
    "\n",
    "### SARSA \n",
    "\n",
    "- 동일한 구조하에서 Q function 만 다르게 \n",
    "\n",
    "### Optimal Policy\n",
    "\n",
    "- $Q((s,a)$ : Q 값을  maximing 하는 것\n",
    "  - Q값으로부터 매 순간 최적의 decision 을 내릴 수 있음 (policy)\n",
    "  \n",
    "  \n",
    "\n",
    "### Bellman Optimality Equation\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "\n",
    "## 1. Deep Q-Learning (DQN)\n",
    "\n",
    "- DQN uses\n",
    "\n",
    "### DQN Training\n",
    "\n",
    "- Q-Learning\n",
    "- Loss\n",
    "- Gradient\n",
    "- Update\n",
    "\n",
    "\n",
    "## 2. Policy Gradient \n",
    "\n",
    "\n",
    "### Advantages of Policy Gradient\n",
    "\n",
    "- Better convergence properties\n",
    "- Effective in high-dimensional or continuous action spaces\n",
    "- Can learn stochastic policies\n",
    "\n",
    "- 단점들이 존재하므로 , policy gradient 단독으로는 잘 쓰이지 않음 (다른 트릭과 함께 쓰이는 경우가 많음)\n",
    "  - Q-learing 과 policy gradient 를 조합해서 많이 사용\n",
    "\n",
    "## Actor-Critic Algorithm\n",
    "\n",
    "- Actor (Policy Gradient)\n",
    "- Critic (Q-function)\n",
    "\n",
    "- Asynchronous Advantage Actor Critic (A3C)\n",
    "\n",
    "\n",
    "## References\n",
    "https://github.com/dennybritz/reinforcement-learning <br/>\n",
    "\n",
    "https://github.com/rlcode/reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
