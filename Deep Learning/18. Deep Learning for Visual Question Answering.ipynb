{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Question Answering (VQA)\n",
    "\n",
    "#### Structure\n",
    "\n",
    "\n",
    "- Word embedding reference\n",
    "  - Stanford , Glove : https://nlp.stanford.edu/projects/glove/\n",
    "  - 단점 : dictionary 에 없는 단어는 파악이 어려움\n",
    "  \n",
    "\n",
    "#### Library installation\n",
    "\n",
    "> conda install libgcc  <br/>\n",
    "> pip install opencv-python <br/>\n",
    "> pip install spacy <br/>\n",
    "\n",
    "data : en_glove_cc_300_1m_vectors-1.0.0 파일을 아래 경로에 옮겨 줌\n",
    "> /home/user/anaconda3/lib/python3.6/site-packages/spacy/data\n",
    "\n",
    "\n",
    "#### VQA 구조 구현\n",
    "\n",
    "![](./img/18_vqa_structure.png)\n",
    "\n",
    "![](./img/18_model_vqa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DataScience/DataScience/Study Note/Deep Learning/DLdata\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-98ac10f23364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m### Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG_16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mimage_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN_weights_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# this is standard VGG 16 without the last two layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#import sputnik\n",
    "import cv2, spacy, numpy as np\n",
    "from spacy.en import English\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Input,Reshape, Activation, Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "'''\n",
    "##\n",
    "## open-cv2 install\n",
    "#pip install opencv-python\n",
    "c#onda install libgcc\n",
    "#pip install spacy\n",
    "'''\n",
    "K.set_image_data_format(\"channels_first\") ##\n",
    "\n",
    "## Check proper working directory\n",
    "path = os.getcwd()\n",
    "os.chdir(path)\n",
    "if os.getcwd().split('/')[-1] == 'DLdata':\n",
    "    pass\n",
    "else:\n",
    "    path = os.getcwd()+'/DLdata'\n",
    "    #raise OSError('Check current working directory.\\n'\n",
    "    #              'If not specified as instructed, '\n",
    "    #              'more errors will occur throught the code.\\n'\n",
    "    #              '- Current working directory: %s' % os.getcwd())\n",
    "print(path)\n",
    "\n",
    "\n",
    "## weight path\n",
    "VQA_model_file_name      = path+'/models/VQA/VQA_MODEL.json'\n",
    "VQA_weights_file_name   = path+'/models/VQA/VQA_MODEL_WEIGHTS.hdf5'\n",
    "label_encoder_file_name  = path+'/models/VQA/FULL_labelencoder_trainval.pkl'\n",
    "CNN_weights_file_name   = path+'/models/CNN/vgg16_weights_th_dim_ordering_th_kernels.h5'\n",
    "\n",
    "\n",
    "'''\n",
    "## download VGG16 weight file\n",
    "https://github.com/fchollet/deep-learning-models/releases\n",
    "filename : vgg16_weights_th_dim_ordering_th_kernels.h5\n",
    "\n",
    "## download pre-train word2vec model\n",
    "$sputnik --name spacy --repository-url http://index.spacy.io install en_glove_cc_300_1m_vectors\n",
    "\n",
    "## copy\n",
    "/home/user/anaconda3/lib/python3.6/site-packages/spacy/data\n",
    "'''\n",
    "\n",
    "### Compile the model\n",
    "from models.CNN.VGG import VGG_16\n",
    "image_model = VGG_16(CNN_weights_file_name)\n",
    "    # this is standard VGG 16 without the last two layers\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "image_model.summary()\n",
    "\n",
    "\n",
    "### Extract Image features from URL\n",
    "def get_image_features(image_file_name, CNN_weights_file_name):\n",
    "    image_features = np.zeros((1, 4096)) # 4096 dimensional 1 image\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (224, 224)) # (w*h)\n",
    "    # img loading using opencv library \n",
    "\n",
    "    ## for URL\n",
    "    #from skimage import io\n",
    "    #im = cv2.resize(io.imread(image_file_name),(224,224))\n",
    "    # dimension setting (224,224,3) to (1,224,224,3) idx1 : 1 image\n",
    "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "\n",
    "    image_features[0,:] = image_model.predict(im)[0]\n",
    "    return image_features\n",
    "\n",
    "\n",
    "### word embedding\n",
    "def get_question_features(question):\n",
    "    #spacy.set_lang_class('en_glove_cc_300_1m_vectors', 'vectors')\n",
    "    word_embeddings = spacy.load('en',vectors = 'en_glove_cc_300_1m_vectors')\n",
    "    tokens = word_embeddings(question)\n",
    "\n",
    "    question_tensor = np.zeros((1, 30, 300))\n",
    "    for j in range(len(tokens)):\n",
    "            question_tensor[0,j,:] = tokens[j].vector\n",
    "    return question_tensor\n",
    "\n",
    "### try the embedding\n",
    "\n",
    "## for windows\n",
    "#spacy.set_lang_class('en_glove_cc_300_1m_vectors', 'vectors')\n",
    "word_embeddings = spacy.load('en',vectors = 'en_glove_cc_300_1m_vectors')\n",
    "\n",
    "obama = word_embeddings(u\"obama\")\n",
    "putin = word_embeddings(u\"putin\")\n",
    "banana = word_embeddings(u\"banana\")\n",
    "monkey = word_embeddings(u\"monkey\")\n",
    "\n",
    "obama.similarity(putin)\n",
    "obama.similarity(banana)\n",
    "banana.similarity(monkey)\n",
    "\n",
    "\n",
    "### VQA model\n",
    "# Image model\n",
    "image_input = Input(shape=(4096,))\n",
    "#model_image = Reshape([4096,])(image_input)\n",
    "\n",
    "# Language Model\n",
    "language_input = Input(shape = (30, 300,)) \n",
    "# (sequence, dimension: word 2 output vector- 300, )\n",
    "# sequence (length: input word coount)\n",
    "model_language = LSTM(512, return_sequences= True)(language_input)\n",
    "model_language = LSTM(512, return_sequences= True)(model_language)\n",
    "model_language = LSTM(512, return_sequences= False)(model_language)\n",
    "\n",
    "# combine model\n",
    "vqa_input = Concatenate()([model_language, image_input])\n",
    "\n",
    "vqa_model = Dense(1024, kernel_initializer='glorot_normal')(vqa_input)\n",
    "vqa_model = Activation('tanh')(vqa_model)\n",
    "vqa_model = Dropout(0.5)(vqa_model)\n",
    "\n",
    "vqa_model = Dense(1024, kernel_initializer='glorot_normal')(vqa_model)\n",
    "vqa_model = Activation('tanh')(vqa_model)\n",
    "vqa_model = Dropout(0.5)(vqa_model)\n",
    "\n",
    "vqa_model = Dense(1024, kernel_initializer='glorot_normal')(vqa_model)\n",
    "vqa_model = Activation('tanh')(vqa_model)\n",
    "vqa_model = Dropout(0.5)(vqa_model)\n",
    "\n",
    "vqa_model = Dense(1000)(vqa_model)\n",
    "vqa_value = Activation('softmax')(vqa_model)\n",
    "\n",
    "vqa_model = Model(inputs = [language_input, image_input], outputs = vqa_value)\n",
    "vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "vqa_model.load_weights(VQA_weights_file_name)\n",
    "vqa_model.summary()\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "#image_file_name = path+\"/test.jpg\"\n",
    "image_file_name = path+\"/food.jpeg\"\n",
    "question = u'what is vehicle in the picture?'\n",
    "\n",
    "## get the image features\n",
    "image_features = get_image_features(image_file_name, CNN_weights_file_name)\n",
    "\n",
    "## get the question features\n",
    "question_features = get_question_features(question)\n",
    "\n",
    "labelencoder = joblib.load(label_encoder_file_name)\n",
    "\n",
    "y_output = vqa_model.predict([question_features,image_features])\n",
    "for label in reversed(np.argsort(y_output)[0,-10:]):\n",
    "    print (str(round(y_output[0,label]*100,2)),\"% \",\n",
    "                     labelencoder.inverse_transform(label))\n",
    "\n",
    "\n",
    "# jupyter 에서 모듈을 못불러 오는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행결과 from Spyder\n",
    "\n",
    "![](./img/18_spyder_output_01.png)\n",
    "\n",
    "![](./img/18_spyder_output_02.png)\n",
    "\n",
    "![](./img/18_spyder_output_03.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
