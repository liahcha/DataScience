{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reinforcement Learning (강화학습)\n",
    "\n",
    "> **RL course by David Silver **<br/>\n",
    "> - Course home : http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html <br/>\n",
    "> - Youtube : https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "\n",
    "- Agent : 모델\n",
    "- Environment : 데이터 (state, reward)\n",
    "\n",
    "![](./img/23_rf.png)\n",
    "\n",
    "- 지도학습(supervised learning) 모델과의 차이점\n",
    "  - Non-iid samples \n",
    "  - 연속적인 의사결정에 활용\n",
    "  - 보상이 즉각적이지 않을 수 있음\n",
    "  \n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "\n",
    "- Reinforcement Learning 에 Depp Learing을 적용한 것\n",
    "- **Deep Q-Learning** : Value 기반 방법\n",
    "  > Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533 <br/>\n",
    "  > - https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html <br/> <br/>\n",
    "  \n",
    "  > Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 <br/>\n",
    "  > - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf <br/>\n",
    "  \n",
    "- **Policy Gradient** : Policy 기반 방법\n",
    "  > Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15) (pp. 1889-1897) <br/> \n",
    "  > - https://arxiv.org/abs/1502.05477 <br/> <br/>\n",
    "  \n",
    "  > Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., ... & Kavukcuoglu, K. (2016, June). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (pp.1928-1937) <br/>\n",
    "  > - https://arxiv.org/abs/1602.01783 <br/>\n",
    "  \n",
    "- 주로 게임환경을 이용하여 연구개발\n",
    "- Input : Raw-pixel frame (State가 매우 많음)\n",
    "- Output : Action (게임조작)\n",
    "\n",
    "\n",
    "## Markov Decision Process (MDP)\n",
    "\n",
    "- $S$ : states (e.g. 게임화면)\n",
    "- $A$ : action (e.g. 조이스틱 & 버튼)\n",
    "- $P_{SA}$ : statae transition probabilities $(s_{t+1} = f(s_{t}, a_{t}))$\n",
    "- $\\gamma$ : discount factor : 너무 많은 단계를 걸쳐서 가계되면 1보다 작은 값을 줌 (빠르게 clear 하기 위해서)\n",
    "- $R$ : reward function (e.g. 게임점수)\n",
    "\n",
    "![](./img/23_mdp.JPG)\n",
    "\n",
    "- Definition\n",
    "> A _Markov DEcision Precess_ is a tuple $<S, A >$ <br/>\n",
    ">   - S : finite set of states <br/>\n",
    "\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "- A state $S_{t}$ is *Markov* if and only if <br/> <br/> \n",
    "$$ \\mathbb{P}[S_{t+1}|S_{t}] = \\mathbb{P}[S_{t+1} | S_{1},  \\cdots , S_{t}]$$\n",
    "- 다음 상태로 갈 때는, 현재 정보가 가장 좋은 정보\n",
    "  \n",
    "  \n",
    "### Markov Process\n",
    "\n",
    "- A _Markov Process_ (or _Markov Chain_) is a tuple $<S, P>$\n",
    "  - $S$ is a (finite) set of states\n",
    "  - $P$ is a state transition probability matrix, <br/>\n",
    "    $P_{ss^{'}} = \\mathbb{P}[S_{t+1}=s^{'}|S_{t}=s]$\n",
    "\n",
    "### Markov Reward Process\n",
    "- A _Markov Reward Process_ is a tuple $<S, P, {\\color{Red} R, \\color{Red} \\gamma}>$\n",
    "  - $S$ is a (finite) set of states\n",
    "  - $P$ is a state transition probability matrix, <br/>\n",
    "    $P_{ss^{'}} = \\mathbb{P}[S_{t+1}=s^{'}|S_{t}=s]$\n",
    "  - ${\\color{Red} R}$ is a reward function, \n",
    "  $\\color{Red}{R_{s} = \\mathbb{E} [R_{t+1}|S_{t}=s]}$\n",
    "  - $\\color{Red} \\gamma$ is a discount factor, $\\color{Red}{\\gamma\\in[0,1]}$\n",
    "\n",
    "### Return\n",
    "- The _return_ $G_{t}$ is the total discounted reward from time-step $t$\n",
    "  - reward 들의 총합\n",
    "  - return 을 maximize 하는 것이 목표\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+1}$$\n",
    "\n",
    "\n",
    "### MDP\n",
    "- A Markov decision process (MDP) is a Markov reward process with decisions. It is an _environment_ in which all states are Markov\n",
    "\n",
    "- A _Markov Decision Process_ os a tuple $<S, \\color{Red}A, P, R, \\gamma>$\n",
    "  - $S$ is a (finite) set of states\n",
    "  - $\\color{Red}A$ is a finite set of actions\n",
    "  - $P$ is a state transition probability matrix, <br/>\n",
    "    $P^{\\color{Red}a}_{ss^{'}} = \\mathbb{P}[S_{t+1}=s^{'}|S_{t}=s, A_{t} = \\color{Red}a]$\n",
    "  - $R$ is a reward function, $R^{\\color{Red}a}_{s} = \\mathbb{E} [R_{t+1} | S_{t} = s, A_{t} = \\color{Red}a]$\n",
    "  - $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "\n",
    "## Policy\n",
    "- A policy $\\pi$ is a distribution over actions given states,  \n",
    "  - state 가 주어졌을때, Action 을 취할 확률\n",
    "  - Return 을 최대화 하는 policy 가 가장 좋은 policy \n",
    "\n",
    "$$\\pi (a | s) = \\mathbb{P}[A_{t} = a | S_{t} = s] $$\n",
    "\n",
    "\n",
    "\n",
    "## Action-Value Function\n",
    "\n",
    "- $G_{t}$ return 의 총합\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{pi}[G_{t} | S_{t} = s]$$\n",
    "<br/>\n",
    "\n",
    "- $q_{\\pi} (s, a) $ : 현재 상태  t 에서, action $\\partial$ 을 취했을때 Return 의 기대값 <br/><br/>\n",
    "\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi} [G_{t} | S_{t} = s, A_{t} = a]$$\n",
    "\n",
    "## Bellmann Equation\n",
    "\n",
    "- Value Function\n",
    "![](./img/23_be_01.JPG)\n",
    "\n",
    "### Bellmann Expectation Equations\n",
    "\n",
    "![](./img/23_be_02.JPG)\n",
    "![](./img/23_be_03.JPG)\n",
    "\n",
    "### Bellman Expectation Equations (SARSA) \n",
    "![](./img/23_be_04.JPG)\n",
    "\n",
    "- 동일한 구조하에서 Q function 만 다르게 \n",
    "\n",
    "\n",
    "### Optimal Policy\n",
    "\n",
    "- $Q((s,a)$ : Q 값을  maximing 하는 것\n",
    "  - Q값으로부터 매 순간 최적의 decision 을 내릴 수 있음 (policy)\n",
    "$${{\\pi_{*} (a|s) = \\left\\{\\begin{matrix}\n",
    "1 \\qquad if a = arg_{a \\in A}\\; max  \\; q_{*} (s,a))\\\\ \n",
    "0 \\qquad otherwise \\qquad \\qquad \\qquad  \\end{matrix}\\right.}} $$  \n",
    "  \n",
    "\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "![](./img/23_qlearning.JPG)\n",
    "\n",
    "\n",
    "## 1. Deep Q-Learning (DQN)\n",
    "\n",
    "- DQN uses **experience replay** and **fixed Q-targets**\n",
    "  - Take action $a_{t}$ according to $\\epsilon$-greedy policy\n",
    "  - Store transition $(s_{t}, a_{t}, r_{t+1}, s_{t+1})$ in replay memory $D$\n",
    "  - Sample random mini-batch of transitions $(s, a, r, s^{'}$ from $D$\n",
    "  - Compute Q-learning targets w.r.t. old, fixed parameters $w^{-}$\n",
    "  - Optimise MSE between Q-network and Q-learning targets\n",
    "  $$L_{i}(w_{i}) = \\mathbb{E}_{s, a, r, s^{'}~D_{i}} \\begin{bmatrix}\n",
    "(r + \\gamma \\; max_{a^{'}}Q(s^{'}, a^{'};w_{i}^{-})-Q(s;a;w_{i}))^{2}\n",
    "\\end{bmatrix}$$\n",
    "  - Using vvarient of stochastic gradient descent\n",
    "\n",
    "## DQN Training\n",
    "\n",
    "#### - Q-Learning\n",
    "$$q_{pi}(s,a)\\approx \\color{Red}{Q(s,a,w)}$$\n",
    "\n",
    "#### - Loss\n",
    "$$\\mathcal{L}(w_{i}) = E[\\color{Blue}{(R(s) + \\gamma \\; max_{a^{'}} \\; Q(s^{'},a^{'},w_{i-1})} - \\color{Red}{Q(s,a,w_{i})})^{2})]$$\n",
    "\n",
    "#### - Gradient\n",
    "\n",
    "$$\\nabla_{w_{i}}\\mathcal{L}(w_{i}) \n",
    "= E[\\color{Blue}{(r+ \\gamma \\; max_{a^{'}} \\; Q(s^{'},a^{'},w_{i-1})} - \\color{Red}{Q(s,a,w_{i})}))^{\\nabla_{w_{i}}Q(s,a,w_{i})}]$$\n",
    "\n",
    "#### - Update\n",
    "$$w_{i+1} \\leftarrow w_{i} - \\alpha \\nabla_{w_{i}} \\mathcal{L} (w_{i}))$$\n",
    "\n",
    "## DQN의 구조\n",
    "\n",
    "- Input : 210 $\\times$ 160 pixel (128 bit) $\\rightarrow$ 84$\\times$84$\\times$4 (down-sampled image & 4 frames)\n",
    " \n",
    "- Output : 각각의 action 에 대응하는 Q-values\n",
    "\n",
    "\n",
    "## 2. Policy Gradient \n",
    "- 어떤 action 을 취할지 policy 를 학습하는 강화학습 방법(policy network)\n",
    "- **Policy **\n",
    "  - Function $\\pi : S \\rightarrow A$\n",
    "  - $\\pi(s,a) = P[a|s, \\theta]$\n",
    "  \n",
    "  ![](./img/23_policy_gradient.JPG)\n",
    "\n",
    "### Advantages of Policy Gradient\n",
    "\n",
    "- Better convergence properties\n",
    "- Effective in high-dimensional or continuous action spaces\n",
    "- Can learn stochastic policies\n",
    "<br/><br/>\n",
    "\n",
    "- 단점들이 존재하므로 , policy gradient 단독으로는 잘 쓰이지 않음 (다른 트릭과 함께 쓰이는 경우가 많음)\n",
    "  - Q-learing 과 policy gradient 를 조합해서 많이 사용\n",
    "\n",
    "### Policy Network의 학습\n",
    "- 강화학습에서는 명시적인 레이블(action)이 존재하지 않음\n",
    "- 게임의 승패를 정답으로 가정하여 gradient 계산\n",
    "\n",
    "![](./img/23_policy_network.JPG)\n",
    "- http://karpathy.github.io/2016/05/31/rl/\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "- For any differentable policy $\\pi_{\\theta}(s,a)$ , <br/>\n",
    "  for any of the policy objective functions $J = J_{1}, J_{avR}$ or $\\frac{1}{1-\\gamma} \\; J_{avV}$ , <br/>\n",
    "  the policy gradient is\n",
    "  \n",
    "  $$\\nabla_{\\theta} J(\\theta) = \\color{Red}{\\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta} \\; log \\; \\pi_{\\theta}(s,a) \\; Q^{\\pi_{\\theta}}(s,a)]}$$\n",
    "\n",
    "\n",
    "## Actor-Critic Algorithm\n",
    "\n",
    "- Actor (Policy Gradient) : Updates action-value function parameters w\n",
    "- Critic (Q-function) : Updates policy parameters $\\theta$, in direction suggested by critic\n",
    "\n",
    "### 실혐결과\n",
    "- 현재 흐름은 policy gradient 를 이용한 강화학습\n",
    "- 대부분의 연구가 policy gradient 기반으로 진행중\n",
    "![](./img/23_a3c.JPG)\n",
    "  - Asynchronous Advantage Actor Critic (A3C)\n",
    "\n",
    "> Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel. (2016. April). Benchmarking Deep Reinforcement Learning for Continuous Control <br/>\n",
    "> - https://arxiv.org/abs/1604.06778\n",
    "\n",
    "\n",
    "## References\n",
    "https://github.com/dennybritz/reinforcement-learning <br/>\n",
    "\n",
    "https://github.com/rlcode/reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
